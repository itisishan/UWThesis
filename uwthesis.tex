%  ========================================================================
%  Copyright (c) 1985 The University of Washington
%
%  Licensed under the Apache License, Version 2.0 (the "License");
%  you may not use this file except in compliance with the License.
%  You may obtain a copy of the License at
%
%      http://www.apache.org/licenses/LICENSE-2.0
%
%  Unless required by applicable law or agreed to in writing, software
%  distributed under the License is distributed on an "AS IS" BASIS,
%  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
%  See the License for the specific language governing permissions and
%  limitations under the License.
%  ========================================================================
%

% Documentation for University of Washington thesis LaTeX document class
% by Jim Fox
% fox@washington.edu
%
%    Revised 2020/02/24, added \caption()[]{} option.  No ToC.
%
%    Revised for version 2015/03/03 of uwthesis.cls
%    Revised, 2016/11/22, for cleanup of sample copyright and title pages
%
%    This document is contained in a single file ONLY because
%    I wanted to be able to distribute it easily.  A real thesis ought
%    to be contained on many files (e.g., one for each chapter, at least).
%
%    To help you identify the files and sections in this large file
%    I use the string '==========' to identify new files.
%
%    To help you ignore the unusual things I do with this sample document
%    I try to use the notation
%       
%    % --- sample stuff only -----
%    special stuff for my document, but you don't need it in your thesis
%    % --- end-of-sample-stuff ---


%    Printed in twoside style now that that's allowed
%
 
\documentclass [11pt, proquest] {uwthesis}[2020/02/24]
 
%
% The following line would print the thesis in a postscript font 

% \usepackage{natbib}
% \def\bibpreamble{\protect\addcontentsline{toc}{chapter}{Bibliography}}

\setcounter{tocdepth}{1}  % Print the chapter and sections to the toc
 

% ==========   Local defs and mods
%

% --- sample stuff only -----
% These format the sample code in this document

\usepackage{alltt}  % 
\newenvironment{demo}
  {\begin{alltt}\leftskip3em
     \def\\{\ttfamily\char`\\}%
     \def\{{\ttfamily\char`\{}%
     \def\}{\ttfamily\char`\}}}
  {\end{alltt}}
 
% metafont font.  If logo not available, use the second form
%
% \font\mffont=logosl10 scaled\magstep1
\let\mffont=\sf
% --- end-of-sample-stuff ---
 



\begin{document}
 
% ==========   Preliminary pages
%
% ( revised 2012 for electronic submission )
%

\prelimpages
 
%
% ----- copyright and title pages
%
\Title{Spatial Computing in Context}
\Author{Ishan Chatterjee}
\Year{2023}
\Program{Computer Science and Engineering}

\Chair{Shwetak Patel}{Professor}{Computer Science \& Engineering}
\Signature{Vikram Iyer}
\Signature{Steve Seitz}
\Signature{Jacob Wobbrock}


\titlepage  

\copyrightpage

 
%
% ----- signature and quoteslip are gone
%

%
% ----- abstract
%


\setcounter{page}{-1}
\abstract{%
The desktop computing paradigm has revolutionized the way we live, work, and communicate, but is limited by the physical boundaries of the devices themselves. More recently, mobile computing has allowed us to take these capabilities on-the-go, but input and output mechanisms remain tied to the device surface, without interplay with the physicality of the surrounding environment. Spatial computing, however, seeks to define a new generation of computing, where users can receive digital guidance, communication, and contextual references based on -- and embedded within -- the environment around them. This means spatial computing will be used \textit{in-context}, where the users are performing another task -- one in which computing can either actively or passively provide support. This provides the new challenge in which the interaction and input of these systems must function within an ongoing situation, but also provides an opportunity where systems can leverage the spatial dimension for additional information to drive natural interaction. I approach this area on three fronts: \textit{spatial computing interaction}, \textit{spatial computing sensing}, and \textit{spatial computing control}.

From the perspective of spatial computing interaction, we investigate how electrical engineers can benefit from spatial augmentations while debugging printed circuit boards and develop an AR workbench that supports them as they work. For spatial computing sensing, we design earbuds that filter out the unwanted commotion in the users’ noisy environment by leveraging the spatial information of interfering audio sources. Finally, for spatial computing control, we develop thumb-to-index finger microgesture interfaces which can allow for control of an augmented reality headset while maintaining the subtlety and precision needed for on-the-go, public scenarios.
}
 
%
% ----- contents & etc.
%
\tableofcontents
\listoffigures
%\listoftables  % I have no tables
 
%
% ----- glossary 
%
% \chapter*{Glossary}      % starred form omits the `chapter x'
% \addcontentsline{toc}{chapter}{Glossary}
% \thispagestyle{plain}
% %
% \begin{glossary}
% \item[argument] replacement text which customizes a \LaTeX\ macro for
% each particular usage.
% \item[back-up] a copy of a file to be used when catastrophe strikes
% the original.  People who make no back-ups deserve
% no sympathy.
% \item[control sequence] the normal form of a command to \LaTeX.
% \item[delimiter] something, often a character, that indicates
% the beginning and ending of an argument.
% More generally, a delimiter is a field separator.
% \item[document class] a file of macros that tailors \LaTeX\ for
% a particular document.  The macros described by this thesis
% constitute a document class.
% \item[document option] a macro or file of macros
% that further modifies \LaTeX\ for
% a particular document.  The option {\tt[chapternotes]}
% constitutes a document option.
% \item[figure] illustrated material, including graphs,
% diagrams, drawings and photographs.
% \item[font] a character set (the alphabet plus digits
% and special symbols) of a particular size and style.  A couple of fonts
% used in this thesis are twelve point roman and {\sl twelve point roman
% slanted}.
% \item[footnote] a note placed at the bottom of a page, end of a chapter,
% or end of a thesis that comments on or cites a reference
% for a designated part of the text.
% \item[formatter] (as opposed to a word-processor) arranges printed
% material according to instructions embedded in the text.
% A word-processor, on the other hand, is normally controlled
% by keyboard strokes that move text about on a display.
% \item[\LaTeX] simply the ultimate in computerized typesetting.
% \item[macro]  a complex control sequence composed of 
% other control sequences.
% \item[pica] an archaic unit of length.  One pica is twelve points and
% six picas is about an inch.
% \item[point] a unit of length.  72.27 points equals one inch.
% \item[roman]  a conventional printing typestyle using serifs.
% the decorations on the ends of letter strokes.
% This thesis is set in roman type.
% \item[rule] a straight printed line; e.g., \hrulefill.
% \item[serif] the decoration at the ends of letter strokes.
% \item[table] information placed in a columnar arrangement.
% \item[thesis] either a master's thesis or a doctoral dissertation.
% This document also refers to itself as a thesis, although it
% really is not one.
 
% \end{glossary}
 
%
% ----- acknowledgments
%
% \acknowledgments{% \vskip2pc
%   % {\narrower\noindent
%   The author wishes to express sincere appreciation to
%   University of Washington, where he has had the opportunity
%   to work with the \TeX\ formatting system,
%   and to the author of \TeX, Donald Knuth, {\it il miglior fabbro}.
%   % \par}
% }

%
% ----- dedication
%
%\dedication{\begin{center}to my dear wife, Joanna\end{center}}

%
% end of the preliminary pages
 
 
 
%
% ==========      Text pages
%

\textpages
 
% ========== Chapter 1
 
\chapter {Introduction}

\section{Spatial Computing}
 
Computing as we know it today is a physically confined experience, most commonly expressed as a box on one's desk or a slab within one's pocket. Interaction with these devices is tied to the device themselves without reference to the user’s environment. The spatial computing paradigm seeks to enable input and output that move beyond the borders of the physical device, sensing and/or augmenting aspects of the surrounding physical world. In this proposal I adopt Greenwold’s original definition of \textit{spatial computing} as "human interaction with a machine in which the machine retains and manipulates referents to real objects and spaces." (This definition also subsumes the more recent uses of spatial computing in marketing material as anything to do with AR/VR/MR.) This aspect of being rooted in the real-world lends spatial computing toward tasks that occur beyond the office desk. For example, the second-generation of Microsoft HoloLens and Magic Leap augmented reality (AR) devices were largely marketed toward industrial applications such as manufacturing line guidance, surgical operations or in-field visualizations. These types of tasks require spatial computing technologies to work \textit{in-context} -- honoring the user’s current workflow or situation. 
 
 
\section{In-Context Computing}
 
Dey and Abowd use \textit{context} to refer to “any information that can be used to characterize the situation of an entity. An entity is a person, place, or object that is considered relevant to the interaction between a user and an application, including the user and applications themselves.” While this definition is then leveraged for defining \textit{context-aware} computing, applications that are actively responsive to changes in context, in this proposal I instead focus in on what I call \textit{in-context} computing. In-context computing is used to assist the user in an application that exists beyond the computer itself, such as while they performing another task or in a particular environment. For example, in-context computing could be an in-flight computer display meant to relay information to a pilot while flying or a conversational voice assistant for navigation queries as the user is walking. (This would be in contrast to word processing on a laptop computer or a mobile game on a smartphone where the application and user task are one and the same, and the application has no relation to the user’s situation or environment.) In-context computing can offer assistive extensions of the human’s capability to allow for multi-tasking or greater efficiency in the task at hand. 
 
 
\section{Putting It Together}
 
As they are both tied to the environment and task-at-hand, spatial and in-context computing are two sides of the same coin. I posit that by implicitly leveraging the \textit{spatial} context of a user’s environment, technologies can better serve users as they operate within a situation, for example, while participating in another task or operating while mobile. Through system building, I explore design constraints and considerations that follow from making spatial computing operate within a given situation, environment, or task. For instance, the latency, precision of interaction, form factor, robustness, power, social acceptability, comfort, and intrusiveness are a handful of key constraints that arise when designing an in-context computing system. For a holistic perspective, I approach this issue from three angles: \textit{spatial computing interaction}, \textit{spatial computing sensing}, and \textit{spatial computing control}.

\subsubsection{Spatial computing interaction:} Beginning with spatial computing interaction, the focus is on co-designing spatial input and output mechanisms that align with the user's workflow, enabling seamless support within their work context. Specifically, in Augmented Reality Debugging Workbench (Chapter 3), we ask: can augmented reality assist electrical engineers in their printed circuit board debugging workflows and how can it best support them? (\textbf{RQ1}) To answer this question, we design and develop an augmented reality workbench for electrical engineers. To facilitate in-context computing, we co-design the interactions around their existing workflows leveraging their workbench surface as a spatial canvas for projection and input.

\subsubsection{Spatial computing sensing:} Moving on to spatial computing sensing, the emphasis lies in understanding and utilizing the spatial characteristics of a given situation to enhance computing performance. Specifically, I focus on spatial audio capture sensing in noisy, real-world environments. In ClearBuds (Chapter 4), we ask: How we can leverage spatial information, in addition to frequency information, about competing environmental sound sources to remove them and allow for clear calls? (\textbf{RQ2}). We develop a pair of wireless earbuds that uses multiple, time-synchronized audio channels and a time-domain network to utilize the spatial distribution of the target speech source and interfering noise sources for speech enhancement.

\subsubsection{Spatial computing control (proposed):} Lastly, attention is given to the control of future spatial computing devices, particularly in mobile scenarios. A challenge lies in designing input modalities for smartglasses that are ergonomic, socially acceptable, and precise, without relying on an external surface. In ToFRing and Placeholder (Chapter 5), we propose to develop two systems to investigate: how can thumb-to-finger microgesture inputs be sensed from a wearable ring and wristband respectively? (\textbf{RQ3}) In ToF Ring, we plan to develop a system that can gather thumb-to-index swipe and thumb motion information in two axes using miniaturized time-of-flight and bio-acoustic sensors. In Placeholder, we plan to leverage electrical sensing to be able to sense thumb-to-index gestures from a wrist-based system, allowing for users who prefer bracelets or watches to similarly be able to control smartglasses. In both systems, the design constraints applied by on-the-go usage influences the interactions and form factors we can consider.

% ========== Chapter 2
 
\chapter{Related Work}
\label{sec:related}

In this section, we review related work for each of the areas of focus: spatial computing interaction (section \ref{sec: Spatial Computing Interaction: Augmented Task Guidance}, sensing, and control. We contextualize the work within the general thesis and then narrow to the specific contributions of the individual works that realize that hypothesis.

\section{Spatial Computing Interaction: Augmented Task Guidance}
\label{sec: Spatial Computing Interaction: Augmented Task Guidance}

\subsection{Augmenting Reality}
\label{subsec: Augmented Reality}

Our own senses are mediated by space -- how we see, how we hear, and how we touch. Through our development, we generate natural intuitions about our body's relation to space [Piaget, J. (1954)]. Therefore tapping into these natural faculties can better lend itself to what Weiser envisioned of computing as "an extension of our unconscious." 
%\subsection{Augmented Visualization and Interaction for Spatially-Associated Information Access}
%\label{subsec: Augmented Visualization and Interaction for Spatially-Associated Information Access}
Augmented reality (AR) has long been seen as a paradigm that can decrease the barrier between virtual and physical information transfer [cite Grasping Reality Through Illusion- Interactive Graphics Serving Science, Annotating the real world with knowledge-based graphics on a see-through head-mounted display.].
% above is the big claim about AR, below is breaking that claim down
This transfer process can consist of two components: the presentation of the information to the user, generally in the form of visualizations; and the ability for the user to interact with the visualization, perhaps enabling the user to query for additional information.
% give examples of the first part of the claim
Prior work has shown that AR systems presenting spatially-tracked information, even with no interaction component, can be effective not only in reducing error rate and mental effort across industrial tasks such as order picking \cite{Schwerdtfeger2008SupportingReality} and object assembly \cite{Caudell1992AugmentedProcesses, Tang2003ComparativeAssembly}, but also as a medium for understanding abstract concepts such as how electrons flow through a circuit \cite{Conradi2011FlowElectrons, Chan2013LightUp}.
Feiner et al.’s seminal KARMA system \cite{Feiner1993Knowledge-basedReality} sought to provide augmented instruction in support of complex 3D tasks, for example guiding the user through laser printer maintenance with overlaid leader lines, callouts and arrows bound to different components of the machine. 

% then give examples of making them interactive
Extending AR experiences to enable interaction makes them even more powerful.
Such interactions might enable actions such as the selection of elements in the physical world to be used as part of a virtual tool operation.
Digitaldesk \cite{Wellner1993InteractingDigitaldesk} demonstrated such interactions with examples such as allowing the user to move a number from a physical price list into a virtual calculator.
% we work on both of these aspects of AR and we will talk about how previous people have used both of these aspects of AR wrt PCBs
In this first section on spatial computing interaction, our work explores the design space of both of these aspects of AR -- visualization and interaction -- and how they might enhance how electrical engineers work with PCBs. We focus on this group as we observe their workflow to require frequent context switching between various virual design files and spatial navigation on their physical design.
% In the following section, we discuss how prior work has used these aspects of AR toward working with PCBs.


% separating line hereeeeeeeeeeee



% Digitaldesk \cite{Wellner1993InteractingDigitaldesk} sought to drive productivity workflows by generating virtual proxies for physical objects by augmenting a desk surface with an overhead camera-projector system. Central to the work’s goal is the movement of data or information between the real and virtual environments, such as transferring a number off a physical price list into a virtual calculator.
% % Ishii \cite{Leithinger2011DirectDisplay} work here?
% In particular, AR has been of great interest for information access that carries a spatial correspondence to objects in reality \cite{Azuma1997AReality, Feiner1993Knowledge-basedReality}, such as order picking \cite{Schwerdtfeger2008SupportingReality} and object assembly \cite{Caudell2003AugmentedProcesses}. Tang et al. \cite{Tang2003ComparativeAssembly} found a decrease in error rate and perceived mental effort in assembly tasks with spatially-tracked AR compared to head up display and instruction manual, however noted that calibration, display, and tracking techniques can have a significant effect on assembly task completion time. The design paradigm of rendering abstract information with corresponding spatially-situated objects has been explored for a long time. Feiner et al.’s KARMA system \cite{Feiner1993Knowledge-basedReality} sought to provide augmented instruction in support of complex 3D tasks, for example guiding the user through laser printer maintenance with overlaid leader lines, callouts and arrows bound to different components of the machine. Instructional augmented overlay has been applied to electronics in a number of instances. Projectron Mapping \cite{Akiyama2014ProjectronMapping} seeks to provide a delightful, tangible feedback using projective AR to light components when a virtual circuit is correctly completed by students. Flow of Electrons \cite{Conradi2011FlowExperientially} helps novices learn about circuit topology and function by allowing physical electronics to be virtually connected with projected augmented overlays on a surface, allowing for experimentation, animated feedback (electron flow through connections), and educational nudges from the tutorial mascot. The study revealed augmentation helped to instruct novice users to build circuits, but the linear arrangement of the tutorial did not allow for a transition to autonomous learning. LightUp \cite{Chan2013LightUp:Electronics}, a block-based teaching tool for children, also utilized mobile AR to help show the movement of electrons through the system. Each of these projects indicate augmented reality can be an enriching tool to bridge the gap between abstract concepts/metadata and real-world circuits. Our work seeks to leverage the design paradigm of providing just-in-time, spatially-situated augmented instruction to lower workload, errors, and workload for PCB debugging.

% \subsection{Augmented Reality}
% \label{subsec: Augmented Reality}

% Augmented reality (AR) involves creating an experience in which a user's sense of reality is altered by superimposing a synthetic image onto the user's view of the real-world environment.
% AR is most commonly implemented using head-mounted displays (HMDs) in order to enable mobility, which allows for always-available functionality.
% As a result, HMD-based AR has primarily found applications in industrial settings, such as order picking and part assembly.
% However, HMD-based AR also introduces many technical complexities, such as making the hardware compact enough to be worn on the head, and designing and calibrating a display that effectively renders the composite image.

% For applications that do not require mobility, AR can alternatively be achieved using a static projector.
% Such a setup is not as constrained in terms of physical space, and simplifies the requirements of the display, since the superimposition of the image occurs physically.
% A common use-case for projected-based AR is the desk or workbench.
% DigitalDesk ..., perceptive workbench...

% Our work describes a hypothetical projected-based AR system for augmenting PCBs... explore the design space for faster interactions and information retrieval.

% \subsection{Visualization Tools for Breadboard Designs}
% \label{subsec: Visualization Tools for Breadboard Designs}



% \subsection{Augmenting Breadboards}
% \label{subsec: Augmenting Breadboards}
% \subsubsection{Visualization tools}
% Because of their easy solderless reconfigurability, breadboards are commonly used amongst hobbyists, students, designers, and engineers for prototyping simple, one-off circuits.
% Fritzing \cite{Knorig2009FritzingDesigners} was designed to support designers and artists in capturing breadboard designs virtually as well as translating breadboard designs into layouts.
% Important to the tool is the “breadboard view” which seeks to emulate the tangible experience of physically building circuits by having draggable wires and drop-in, semi-realistic components.
% Similar to how the layout file is a virtual spatial capture of a PCB, the Fritzing diagram serves as a “digital twin” to a real breadboard circuit, providing a virtual copy on which future tools add augmentation.

% % \subsection{Visualization Tools for Breadboard Designs Incorporating Interactivity and Measurement}
% % \label{subsec: Visualization Tools for Breadboard Designs Incorporating Interactivity and Measurement}
% \subsubsection{Adding interactivity and measurement}
% A number of systems have extended the virtual breadboard visualizations to allow for augmented interaction or measurement capability.
% Proxino \cite{Wu2019Proxino:Proxies} extends Fritzing by allowing for physically maniputable sensor proxies that can drive values in a virtual components within a  Fritzing circuit.
% Visible Breadboard \cite{Ochiai2014VisibleElectricity} allows for wiring to be performed with swipes directly on the board, and the connected nodes are highlighted in the same LED color.
% Toastboard \cite{Drew2016TheCircuits} and CurrentViz \cite{Wu2017CurrentViz:Circuits} collect real-time state information of prototyped breadboard circuits on custom instrumented breadboard hardware, specifically voltage and current information respectively.
% Toastboard pushes voltage information into a virtual diagram, and also provides row-associated status LEDs which were cited by all participants as being useful for debugging feedback. In the CurrentViz, current information from an instrumented breadboard is visualized atop a corresponding Fritzing diagram.

% While measurement points can be arbitrarily accessed in breadboarded setups, access points to pull current information must be designed into PCBs a priori, usually in the form of external sense resistors or internal sense resistors within PMICs (power management integrated circuits) or other ICs.
% RwmLAB \cite{Asumadu2003AInstrument}, Scanalog \cite{Strasnick2017Scanalog:Hardware}, VISIR \cite{Tawfik2013VirtualBreadboard} and VirtualComponent \cite{Kim2019VirtualComponent:Circuits} leverage this arbitrary circuit node access allowed by breadboarding to perform component insertion.
% Each tool has an associated breadboard GUI where users can design a virtual circuit and an instrumented breadboard is re-wired to match this digital twin. 
% Measurements can be taken on the physical circuits via the GUI.
% Scanalog and VirtualComponent both allow access to mix physical and virtual components, and VirtualComponent provides a mobile AR based overlay of the virtual components on a video feed of a physical breadboard.
% Each of these tools, while supporting the maker, student, or DIY hobbyist may not support the needs experience in the electronics industry.
% Breadboards are a quick and easy platform to iterate on the topology and values of simple analog and low-speed digital circuits.
% However in industry, circuits are rarely prototyped on breadboards as the circuits are too complex to hand-assemble into a breadboard format, many ICs are not available in DIP packages compatible with breadboarding, and parasitic capacitance in breadboard construction can affect sensitive or high speed signals. 

% \subsection{Visualization Tools for PCB Designs}
% \label{subsec: Visualization Tools for PCB Designs}
\subsection{Limitations of Current Tools}

During the process of debugging a new PCB design, engineers must constantly move between their schematic, layout, and physical representations in order to validate their design or understand the nature of a design failure\footnote{Some helpful technical background and terminology: The electrical engineering design process typically starts with designing a circuit to meet a set of functional requirements.
Using an electronic computer-aided design (ECAD) tool, the logic of the circuit is formalized via schematic capture into a \textit{schematic diagram}, which visualizes the circuit's components as symbols and the circuit's interconnections (\textit{nets}) as topological lines between the components' pins.
This logic is then transferred to a \textit{layout diagram}, where components and connections are placed in a physical coordinate space.
Finally, the design is then physically fabricated and assembled into a functional PCB, where the components are soldered onto the surface of a fiberglass board with conductive pads, vias, and traces running buried within its layers. For images, see online tutorials on Sparkfun (\url{https://learn.sparkfun.com/tutorials/pcb-basics/all}) or Adafruit (\url{https://learn.adafruit.com/making-pcbs-with-oshpark-and-eagle}).}
(Fig. \ref{fig:teaserfigure}).
Through current ECAD tools, this can be a time-consuming and error-prone process which typically involves manually following correspondences and nomenclatures
% demarcated by a component reference designator (such as ‘{\fontfamily{cmtt}\selectfont{R234}}’), net name (‘{\fontfamily{cmtt}\selectfont{PP\_VSYS\_5V}}’), or approximate location on the board,
across different software applications, often using each application’s textual ``find'' command. To locate the corresponding area-of-interest on the board, the engineer must textually match the reference designator against the board’s silkscreen (if printed) or visually pattern match layout representation to the physical PCB, which can become more challenging with dense components or different orientations.
As a debugging procedure typically involves tens to hundreds of these correspondences, this procedure can quickly become tedious with the ever-increasing complexity of board designs only exacerbating the challenge. 
In our work, we interview electrical engineers on their workflow to get a better picture of pain points in their current processes. We design our spatial computing system with an eye towards relieving these pain points.

\subsection{Augmented Reality for PCBs}

While PCBs are often considered the staple of industry level electronics, breadboards are often used by students and hobbyists for their solderless reconfigurability that enables rapid iteration. However, they are rarely used as part of the hardware development process in industry.
While recent work in the HCI community aimed at the student population has demonstrated a number of breadboard augmentation techniques \cite{Ochiai2014VisibleElectricity, Drew2016TheToastboard, Wu2017CurrentViz, Kim2019VirtualComponent}, PCBs are substantially more intricate, requiring much more careful augmentation.
In this section, we discuss more directly comparable related work in the realm of specifically augmenting PCBs.


\subsubsection{Visualization Tools}
A few tools support visualizing certain component metadata, such as location, directly on the PCB.
InspectAR \cite{InspectARTools} is a recently released tool that uses mobile AR to overlay elements of the layout and associated metadata onto a camera view of the PCB displayed on a mobile tablet or PC.
It is targeted toward supporting industry professionals, with couplings to industry standard ECAD tools.
The tool does not seem to support direct interaction with the PCB itself, measurement interactions, or a topological schematic view.
The sales webpage offers strong testimonials speaking to the increased assembly and debugging efficiency from decreased context-switching, claiming “an average 30\% reduction in lab-time.”
While these indications speak strongly to the hypothesis that mixed reality visualization of layout metadata on PCB can increase efficiency, a systematic study is yet to be published.
The Mascot \cite{MascotRobotas}, a robotic workbench from Robotas, helps to support operators performing hand assembly of through hole components
by steering a projected laser spot to the installation location on an anchored PCB.
%The tool allows for preloading of assembly steps, which controls automated picking carousels and a projected laser spot showing assembly position on an anchored PCB.
Similarly, Hahn et al. \cite{Hahn2015AugmentedProcess} generated an AR tool with textual and graphical cues delivered through a smartglass for assisting workers performing PCB assembly, indicating that the tool allowed for errorless part picking and assembly.
Hahn et al.’s tool, InspectAR and Mascot all provide board-locked augmented instruction for PCB workflow, driving information from the virtual design files to the user’s view of the PCB. Our work broadens the design space seeking to also incorporate augmented interaction and measurement to pass data in the opposite direction, that is, interactive capture in the PCB view can be passed to the virtual design files.

% \subsection{Visualization Tools for PCB Designs Incorporating Interactivity and Measurement}
% \label{subsec: Visualization Tools for PCB Designs Incorporating Interactivity and Measurement}
\subsubsection{Adding Interactivity and Measurement}
Pinpoint \cite{Strasnick2019Pinpoint} is a tool designed to assist in PCB debugging by allowing users to modify and measure the circuit \textit{in situ} after the PCB is fabricated. The tool modifies the layout of a PCB by inserting breakable connections on some traces. While not using augmented reality per se, the tool connects the virtual and the physical by using GUI-controlled relays to make and break these connections. For form factor designs and mass-produced PCBs, modifying the layout for test is typically restrained to adding test points on critical nets for bed-of-nails, on-line testing or manual access for workbench debugging. Our work seeks to support existing debugging workflows that do not modify the PCB design, and instead ease access to measurement points by guiding users with augmentations.

%Pinpoint \cite{Strasnick2019Pinpoint:Instrumentation} extends the concept in Scanalog, VISIR, and Virtual Component to PCB designs. The tool adds up to 16 jumper pads into a PCB design before fabrication, generates a bed-of-nails jig to interface with the jumper pads, and modifies connections through a GUI-controlled relay board. While the tool adds a unique layer of debugging capability akin to breadboarding -- the ability to access and isolate parts of a circuit for easy measurement or modification -- it necessitates amending the original design routing. For form factor designs and mass-produced PCBs, a high level of control in layout is maintained by the designer with board size, manufacturing, cost, and signal integrity concerns often precluding the use of an auto routing mechanism. The modifications to layout in production designs in support of test is therefore typically restrained to adding test points on critical nets for bed-of-nails, on-line testing or manual access for workbench debugging. Our work seeks to support existing debugging workflows that do not modify the PCB design, and instead ease access to measurement points by guiding users with augmentations.
More relevant to our work, BoardLab presents a magnetically tracked stylus that enables interactions from board to schematic, such as selecting and identifying components on the schematic by touching the components on the board as well as taking voltage measurements and having the measurement annotated on the schematic \cite{Goyal2013BoardLab}.
Although the system looks promising, no formal evaluation was reported.
Our work studies whether the interactions afforded by such a stylus would be helpful to electrical engineers as they actively debug, as well as exploring interactions that are synergistically enabled as augmented interaction and measurement is paired with simultaneous augmented visualization.


%Augmented Reality (AR) have the potential to be highly adaptable and customizable to individual users and specific use cases, while minimizing physical barriers between virtual and physical information transfer. %Recent work have demonstrated the feasibility and effectiveness of using AR to display information on PCBs~\cite{Chatterjee2021AugmentedBoards}. Augmented Silkscreen presented a set of augmented reality interaction techniques to assist electrical engineers in PCB debugging. They found that combining augmented visualization and augmented interaction on printed circuit boards unlocks promising avenues to alleviate the frequent context switching~\cite{Chatterjee2021AugmentedBoards}. In the reset of this section, we will first focus on related work on enabling better debugging workflow on breadboards and later we provide literature related to extending the capabilities of PCBs.














\section{Spatial Computing Sensing: Spatial Audio Capture} 

\subsection{Sensing Space and Distance}

Historically, systems have used a wide variety of sensing methodologies to understand space. Radar, sonar, and, more recently, GPS were all initially developed for wartime applications with battlefield-scale positioning. Within the room-scale, Sutherland's pioneering augmented reality system explored the use of multi-frequency continuous-wave ultrasound, though the first system relied on a set of taut lines on reels sensed by positional encoders. The calculated device pose was used to update the see-through graphics in real-time. Modern AR and VR head-mounted devices utilize a passive method of 6DOF positioning, derived from simultaneous localization and mapping (SLAM) robotics. Images from multiple on-headset cameras are processed to generate a 3D visual map of world keypoints. Intermediate positions are calculated by integration of a calibrated inertial measurement unit (IMU) and are fused with vision via an Extended Kalman Filter. This technique has been key to allow for these HMDs to be mobile and used in-the-field without need for external tracking hardware. For dense mapping of the geometry of surrounding environment, a number of headsets employ a time-of-flight depth camera which can be leveraged for semantic understanding of the surrounding geometry, accurate interactive plane detection, and hand tracking. %do i need to add polhemus?

With the exception of SLAM, each of these systems utilize precise timings of reflected or transmitted waves with a known velocity to calculate distance. This same technique is utilized in the audio domain to localize sound sources. Our ears are separated by a known baseline. Audio arriving to our ears from an off-center source arrive at slightly different times resulting in an interaural time difference (ITD) given the speed of sound [lord rayleigh duplex theory]. This phenomenon, along with interaural level differences (ILD) and frequency modulations resulting from the head-related transfer function, are the primary cues that allow humans and other animals to precisely localize sound sources.

In noisy environments, like when on a busy street, multiple competing sound sources layer on top one another creating a noisy mixture. Dubbed the cocktail party effect, our brain's ability to focus auditory attention on a given sound source while filtering out other stimulus has been studied since at least the early 1950s. Interaural time differences [cite binaural unmasking] as well as the audio frequency content both play a part in source separation. Here we focus on a producing a system that can utilize both cues together to enhance a user's voice from amongst a noisy background. We apply this technique specifically to a set of wireless earbuds. Because of their portability, these systems are often used to take calls in noisy, dynamic environments. Real-time speech enhancement is a long standing problem in the signal processing and machine learning communities.  While recent advances in the machine learning community have shown promising results, none of them have been demonstrated on wireless earbuds. Further, the vast majority cannot run on mobile devices and meet these real-time constraints. As a result, endfire beamforming configurations remain popular on most consumer mobile phones and earbuds \cite{samsungglobalnewsroom_2014, airpods, sennheiser_2020, beamforming-app-note}. Below, we briefly discuss beamforming, single channel speech enhancement networks, and binaural networks. By creating a wireless acoustic  network between two earbuds and a novel light-weight hybrid  time-domain and spectrogram-based  neural network, we show for the first time that real-time two-channel neural networks can outperform  current real-time speech enhancement approaches for wireless earbuds.

\subsection{Spatial Audio Capture}

\subsubsection{Beamforming techniques.} A common approach to enhancing speech is to design a beamforming microphone array to be more sensitive to sounds coming from the direction of the user's mouth \cite{van1988beamforming} or voice~\cite{dov-uist21}. Since signal-processing based beamforming is computationally lightweight compared to other speech enhancement techniques, these techniques are deployed on many commercially available audio products today such as smart speakers \cite{amazon}, mobile phones \cite{samsungglobalnewsroom_2014}, and earbud devices like Apple AirPods \cite{airpods}. However, the performance of beamforming is limited by the geometry of the microphones and the distance between them \cite{van1988beamforming, InvenSense}. The form factor of devices like AirPods restricts both the number of microphones on a single earbud and the available distance between them, limiting the gain of the beamformer. While beamforming simultaneously across two earbuds could provide better performance in principle, current wireless architectures are limited to streaming from a single earbud at a time \cite{bluetooth}. Furthermore, adaptive beamformers such as MVDR \cite{frost1972MVDR}, while showing promise with relatively few interfering sources,  are sensitive to sensor placement tolerance and steering \cite{zhang2017deep, brandstein2001microphone}. Finally, beamforming leverages spatial cues only and does not use acoustic cues and perceptual differences to discriminate sources, information that machine learning methods leverage successfully.

% Don H. Johnson and Dan E. Dudgeon. Array Signal Processing: Concepts and Techniques. Simon & Schuster, Inc., USA, 1992

% Beamforming microphone arrays for speech enhancement, https://ieeexplore.ieee.org/abstract/document/225915

% Rate-Constrained Beamforming in Binaural Hearing Aids
% https://link.springer.com/content/pdf/10.1155/2009/257197.pdf

% Dual-Channel Speech Enhancement by
% Superdirective Beamforming
% https://link.springer.com/content/pdf/10.1155/ASP/2006/63297.pdf


%\vskip 0.05in\noindent{\bf Single-channel speech enhancement.} 
\subsubsection{Single-channel speech enhancement}
Recent deep learning techniques have led to significant progress in single channel speech enhancement methods. These models typically operate on spectrograms to separate the human voice from background noise \cite{realtimenoise, Mohammadiha_2013, online_nonnegative, nikzad2020deep, choi2019phaseaware, lstm_speechenhancement, fu2019metricgan, TFMasking}. However, recent trends have opted to operate directly on the time domain signals \cite{luo2019conv, germain2018speech, pascual2017segan, demucsreal, macartney2018improved}, yielding performance improvements over spectrogram approaches. Commercially available noise suppression software like Krisp \cite{krisp} and Google Meet \cite{googlemeet} have successfully deployed single-channel models in real-time and are available for use on mobile phones and desktop computers, processing is performed on the cloud. However, single-channel models can not effectively capture the spatial information and hence fail to isolate the intended speaker when there are multiple speakers present.

% --> some recent work on increasing performance to real-time applications
% Real Time Speech Enhancement in the Waveform Domain
% https://arxiv.org/abs/2006.12847

\subsubsection{Multi-channel source separation and speech enhancement}
%\subsection{Multi-channel source separation and speech enhancement}
Multi-channel methods have been shown to perform better than their single-channel source separation counterparts \cite{yoshioka2018multi, chen2018multi, zhang2017deep, gu2020enhancing, tzirakis2021multichannel, jenrungrot2020cone}. Binaural methods have also been used for source separation \cite{binaural1, han2020realtime, li2011two, reindl2010speech} and localization \cite{van2008binaural, lyon1983computational, kock1950binaural}. Our method improves on existing binaural methods by combining time-domain neural network with spectrogram-based frequency masking networks as well as optimizing them to enable   real-time processing on a phone. Recent works~\cite{binaural_osu, dual_phone} use multiple microphones on a smartphone for  speech-enhancement however neither of them demonstrates real-time performance of a smartphone. In contrast, we demonstrate the first system that achieves real-time speech enhancement using microphones on the two wireless earbuds. Further, as the distance between the earbuds is larger than the distance between microphones on a typical mobile phone, we can  attain a better baseline than a mobile phone implementation, while also retaining the ability to speak hands-free.

% \subsubsection{Earable computing and platforms}
% There has been recent interest in earable computing and sensing~\cite{oesense21,esense-1,esense-2,plat-1,romit-1}. Earable platforms such as eSense~\cite{esense-1,esense-2} have enabled research in enabling various sensing applications using earable devices. eSense however does not support time-synchronized audio transmission from two earbuds to a mobile device. This is a critical requirement for achieving  speech enhancement using the  microphones on the two wireless earbuds.  OpenMHA~\cite{open-1,open-2} is a open signal processing {\it software} platform for hearing aid research. In contrast, we create wireless earbud hardware, which will be open-sourced at publication and can support synchronize wireless transmission from the two earbuds to achieve speech enhancement. 

% --> multi-mic
% Cone-of-silence

%Some approaches, such as \cite{binaural_osu, dual_phone} have used the multiple microphones on a smartphone for real-time speech-enhancement. However, these networks would have to be retrained for every mobile phone as the position and distance between these microphones could change with future iterations. As our method is instead deployed on wireless earbuds, the geometry of the speaker in respect to the microphones will remain consistent. Further, as the distance between the earbuds is larger than the distance between microphones on a typical mobile phone, we attain a better baseline than a mobile phone implementation, while also retaining the ability to speak hands-free.











 

\section{Spatial Computing Control: Microgesture Interaction}

As screens have shrunk from desktop computer to mobile phone to smartwatch, the friction to accessing screen content has decreased, but along with that the throughput of delivered content has also decreased. To overcome these limitations, there has a huge amount of investment towards consolidation of display and sensing to a head-mounted device (HMD). A variety of flavors of HMD exist to tackle both sides of the spectrum -- to decrease information friction as well as to deliver greater throughput. Barring engineering limitations, here the display space can one day be infinitely large as the user's field of view. Smartglasses can operate as a head-up display for timely and spatially relevant digital content. Higher-end mixed reality devices can maintain spatial coherence of the augmentations during movement, providing the illusion of world-locked content. Virtual reality devices can provide a sense immersion that transports the user to a new environment.

To be able to control these immersive devices, 3DOF or 6DOF controllers are typically employed. These controllers use outside-in or inside-out camera-based tracking. The controllers offer compelling experiences for high-fidelity immersive gaming,  object position is needed. Augmented and mixed reality devices also offer articulated hand tracking using computer vision on depth or camera data to allow for direct manipulation of spatial content. The interaction technique manipulations are studied in 

However,
Gorilla arm.
for productivity we 

, or recognize elements in its surroundings and apply augmentations within your space.




Direct manipulation
Indirect manipulation (ray casting etc.)
Decoupled 


Commercial hand gesture detection systems \cite{microsoft, MetaStore, TrackingUltraleap} often rely on optical methods, using cameras mounted on external devices such as AR/VR headsets or necklaces. However, these systems require a clear line of sight to the hand, complicating efforts to  detect gestures made outside the camera's field of view.
% Therefore, a number of systems have explored sensors mounted on the wrist, hand, and/or fingers, spanning a wide range of modalities: \red{optical (Digits \cite{Kim2012Digits},
% TouchPoint \cite{Chatterjee2016TouchPoint:Device},
% SensIR \cite{McIntosh2017SensIR:Reflection}, CyclopsRing \cite{Chan2015Cyclopsring:Ring}, PinchWatch \cite{loclair2010pinchwatch}), thermal (Pyro \cite{Gong2017Pyro:Sensing}, ThermalRing \cite{thermalring}), bio-acoustic (Amento et al. \cite{10.1145/506443.506566}, 
% Mujibiya et al. \cite{sot}, 
% FingerPing \cite{fingerping}), 
% ultrasonic (Beamband \cite{Iravantchi2019BeamBand:Beamforming}, SoundTrak \cite{Zhang2017SoundTrak}), mechanical (DataGlove \cite{Takada2019AFiber}, BackHand \cite{Lin2015BackHand:Hand}, 
% Kuno et al. \cite{10.5555/3298830.3298870}), 
% and inertial (Gu et al. \cite{Guy}, TapID \cite{Meier2021TaplD:Sensing}, FingeRing \cite{Fukumoto1994FingeRing:Interface}, DualRing \cite{Liang2021DualRing}).}
Therefore, many systems have explored sensors mounted on the wrist, hand, and/or fingers, spanning a wide range of modalities: \red{optical (\cite{Chatterjee2016TouchPoint:Device}, \cite{Kim2012Digits},
\cite{McIntosh2017SensIR:Reflection}, \cite{Chan2015Cyclopsring:Ring},  \cite{loclair2010pinchwatch}), (\cite{Gong2017Pyro:Sensing}, \cite{thermalring}), bio-acoustic (\cite{10.1145/506443.506566}, \cite{sot}, \cite{fingerping}), 
ultrasonic (\cite{Iravantchi2019BeamBand:Beamforming}, \cite{Zhang2017SoundTrak}), mechanical (\cite{Takada2019AFiber}, \cite{Lin2015BackHand:Hand}, \cite{10.5555/3298830.3298870}), 
and inertial (\cite{Guy}, \cite{Meier2021TaplD:Sensing}, \cite{Fukumoto1994FingeRing:Interface}, \cite{Liang2021DualRing}).}
The capability of these devices ranges from detecting pinches to recognizing discrete gesture sets to driving full kinematic models.

However, these approaches have several limitations: (1) passive IMUs and bio-acoustic techniques can detect when the fingers make contact  but not when they release, which is important for actions like dragging and dropping; (2) IMUs used for gesture sensing require multiple points of instrumentation, making the system awkward to use; (3) optical and ultrasonic techniques require a clear line of sight to each gesturing appendage, limiting possible mounting positions and making it difficult to detect pinches; and (4) mechanical and magnetic systems require instrumentation of the whole hand, back of the hand, or fingertips, making the system uncomfortable to use.
%The limitations of these objects can be roughly categorized into a few areas: (1) passive IMU and bio-acoustic techniques can detect pinch touchdowns, however cannot detect when the fingers release, an important factor as a clutching mechanism, for example, drag-and-drop, (2) when used for gesture sensing, IMUs require multiple instrumentation points yielding an awkward system, (3) optical and ultrasonic techniques require line-of-sight to each of the gesturing appendages resulting non-ideal mounting positions to capture the full gesture set and are challenged to reliably detect pinches, (4) mechanical and magnetic systems require instrumenting the whole hand, back-of-the-hand, or fingertips, yielding an uncomfortable system.
In contrast, Z-Ring instruments the body at only a single location, i.e., the base of the index finger, an ergonomic and socially acceptable area for worn systems. {By using the body as a transmission medium, Z-Ring does not require line-of-sight for microgesture sensing. In addition, it can robustly sense both touchdown and touch up events for one- and two-handed gestures even if touch velocity is low.}
%This motivated other optical methods to  explore mounting on the arm or finger to detect gestures. Digits derives an articulated hand model leveraging a wirst-mounted IR camera and projector \cite{Kim2012Digits}. TouchPoint uses a pair of line sensors for continuous finger tracking on the back-of-the-hand \cite{Chatterjee2016TouchPoint:Device}. Back-Hand-See . CyclopsRing enables gesture sensing an \cite{}.
 
% ========== Chapter 3
 
\chapter{Augmented Task Guidance: AR Debugging Workbench}

% ========== Chapter 4

\chapter{Spatial Audio Capture: ClearBuds}

% ========== Chapter 5

\chapter{Microgesture Interaction: ToFRing and Placeholder}





%\printendnotes
%
% ==========   Bibliography
%
\nocite{*}   % include everything in the uwthesis.bib file
\bibliographystyle{plain}
\bibliography{uwthesis}
%
% ==========   Appendices
%
\appendix
\raggedbottom\sloppy
 
% ========== Appendix A
 
\chapter{Where to find the files}
 


\end{document}
