%  ========================================================================
%  Copyright (c) 1985 The University of Washington
%
%  Licensed under the Apache License, Version 2.0 (the "License");
%  you may not use this file except in compliance with the License.
%  You may obtain a copy of the License at
%
%      http://www.apache.org/licenses/LICENSE-2.0
%
%  Unless required by applicable law or agreed to in writing, software
%  distributed under the License is distributed on an "AS IS" BASIS,
%  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
%  See the License for the specific language governing permissions and
%  limitations under the License.
%  ========================================================================
%

% Documentation for University of Washington thesis LaTeX document class
% by Jim Fox
% fox@washington.edu
%
%    Revised 2020/02/24, added \caption()[]{} option.  No ToC.
%
%    Revised for version 2015/03/03 of uwthesis.cls
%    Revised, 2016/11/22, for cleanup of sample copyright and title pages
%
%    This document is contained in a single file ONLY because
%    I wanted to be able to distribute it easily.  A real thesis ought
%    to be contained on many files (e.g., one for each chapter, at least).
%
%    To help you identify the files and sections in this large file
%    I use the string '==========' to identify new files.
%
%    To help you ignore the unusual things I do with this sample document
%    I try to use the notation
%       
%    % --- sample stuff only -----
%    special stuff for my document, but you don't need it in your thesis
%    % --- end-of-sample-stuff ---


%    Printed in twoside style now that that's allowed
%
 
\documentclass [11pt, proquest] {uwthesis}[2020/02/24]
 
%
% The following line would print the thesis in a postscript font 

% \usepackage{natbib}
% \def\bibpreamble{\protect\addcontentsline{toc}{chapter}{Bibliography}}

\setcounter{tocdepth}{1}  % Print the chapter and sections to the toc
 

% ==========   Local defs and mods
%

% --- sample stuff only -----
% These format the sample code in this document

\usepackage{alltt}  % 
\newenvironment{demo}
  {\begin{alltt}\leftskip3em
     \def\\{\ttfamily\char`\\}%
     \def\{{\ttfamily\char`\{}%
     \def\}{\ttfamily\char`\}}}
  {\end{alltt}}
 
% metafont font.  If logo not available, use the second form
%
% \font\mffont=logosl10 scaled\magstep1
\let\mffont=\sf
% --- end-of-sample-stuff ---
 



\begin{document}
 
% ==========   Preliminary pages
%
% ( revised 2012 for electronic submission )
%

\prelimpages
 
%
% ----- copyright and title pages
%
\Title{Spatial Computing in Context}
\Author{Ishan Chatterjee}
\Year{2023}
\Program{Computer Science and Engineering}

\Chair{Shwetak Patel}{Professor}{Computer Science \& Engineering}
\Signature{Vikram Iyer}
\Signature{Steve Seitz}
\Signature{Jacob Wobbrock}


\titlepage  

\copyrightpage

 
%
% ----- signature and quoteslip are gone
%

%
% ----- abstract
%


\setcounter{page}{-1}
\abstract{%
The desktop computing paradigm has revolutionized the way we live, work, and communicate, but is limited by the physical boundaries of the devices themselves. More recently, mobile computing has allowed us to take these capabilities on-the-go, but input and output mechanisms remain tied to the device surface, without interplay with the physicality of the surrounding environment. Spatial computing, however, seeks to define a new generation of computing, where users can receive digital guidance, communication, and contextual references based on -- and embedded within -- the environment around them. This means spatial computing will be used \textit{in-context}, where the users are performing another task -- one in which computing can either actively or passively provide support. This provides the new challenge in which the interaction and input of these systems must function within an ongoing situation, but also provides an opportunity where systems can leverage the spatial dimension for additional information to drive natural interaction. I approach this area on three fronts: \textit{spatial computing interaction}, \textit{spatial computing sensing}, and \textit{spatial computing control}.

From the perspective of spatial computing interaction, we investigate how electrical engineers can benefit from spatial augmentations while debugging printed circuit boards and develop an AR workbench that supports them as they work. For spatial computing sensing, we design earbuds that filter out the unwanted commotion in the users’ noisy environment by leveraging the spatial information of interfering audio sources. Finally, for spatial computing control, we propose a thumb-to-index finger microgesture interface which can allow for control of an augmented reality headset while maintaining the subtlety and precision needed for on-the-go, public scenarios.
}
 
%
% ----- contents & etc.
%
\tableofcontents
\listoffigures
%\listoftables  % I have no tables
 
%
% ----- glossary 
%
% \chapter*{Glossary}      % starred form omits the `chapter x'
% \addcontentsline{toc}{chapter}{Glossary}
% \thispagestyle{plain}
% %
% \begin{glossary}
% \item[argument] replacement text which customizes a \LaTeX\ macro for
% each particular usage.
% \item[back-up] a copy of a file to be used when catastrophe strikes
% the original.  People who make no back-ups deserve
% no sympathy.
% \item[control sequence] the normal form of a command to \LaTeX.
% \item[delimiter] something, often a character, that indicates
% the beginning and ending of an argument.
% More generally, a delimiter is a field separator.
% \item[document class] a file of macros that tailors \LaTeX\ for
% a particular document.  The macros described by this thesis
% constitute a document class.
% \item[document option] a macro or file of macros
% that further modifies \LaTeX\ for
% a particular document.  The option {\tt[chapternotes]}
% constitutes a document option.
% \item[figure] illustrated material, including graphs,
% diagrams, drawings and photographs.
% \item[font] a character set (the alphabet plus digits
% and special symbols) of a particular size and style.  A couple of fonts
% used in this thesis are twelve point roman and {\sl twelve point roman
% slanted}.
% \item[footnote] a note placed at the bottom of a page, end of a chapter,
% or end of a thesis that comments on or cites a reference
% for a designated part of the text.
% \item[formatter] (as opposed to a word-processor) arranges printed
% material according to instructions embedded in the text.
% A word-processor, on the other hand, is normally controlled
% by keyboard strokes that move text about on a display.
% \item[\LaTeX] simply the ultimate in computerized typesetting.
% \item[macro]  a complex control sequence composed of 
% other control sequences.
% \item[pica] an archaic unit of length.  One pica is twelve points and
% six picas is about an inch.
% \item[point] a unit of length.  72.27 points equals one inch.
% \item[roman]  a conventional printing typestyle using serifs.
% the decorations on the ends of letter strokes.
% This thesis is set in roman type.
% \item[rule] a straight printed line; e.g., \hrulefill.
% \item[serif] the decoration at the ends of letter strokes.
% \item[table] information placed in a columnar arrangement.
% \item[thesis] either a master's thesis or a doctoral dissertation.
% This document also refers to itself as a thesis, although it
% really is not one.
 
% \end{glossary}
 
%
% ----- acknowledgments
%
% \acknowledgments{% \vskip2pc
%   % {\narrower\noindent
%   The author wishes to express sincere appreciation to
%   University of Washington, where he has had the opportunity
%   to work with the \TeX\ formatting system,
%   and to the author of \TeX, Donald Knuth, {\it il miglior fabbro}.
%   % \par}
% }

%
% ----- dedication
%
%\dedication{\begin{center}to my dear wife, Joanna\end{center}}

%
% end of the preliminary pages
 
 
 
%
% ==========      Text pages
%

\textpages
 
% ========== Chapter 1
 
\chapter {Introduction}

%\section{Spatial Computing}
 
%Computing as we know it today is a physically confined experience, most commonly expressed as a box on one's desk or a slab within one's pocket. Interaction with these devices is tied to the device themselves without reference to the user’s environment. The spatial computing paradigm seeks to enable input and output that move beyond the borders of the physical device, sensing and/or augmenting aspects of the surrounding physical world. Rather than demanding the users entire time and attention, this allows users to interact and perform tasks in the world, opening the door to computing acting as Weiser envisioned it: "a helpful, invisible servant".

Traditionally, computing has been associated with a confined experience, where users interact with devices that are stationary on a desk or handheld in their pockets. Interaction with these devices is tied to the device themselves without reference to the user’s environment. In this conventional setup, the focus is solely on the device itself, demanding the user's full attention. The spatial computing paradigm seeks to enable input and output that moves beyond the borders of the physical device, sensing and/or augmenting aspects of the surrounding physical world. Rather than demanding the users entire focus, this allows users to interact and perform tasks in the world with computing acting as a seamlessly integrated and helpful assistant, as Weiser once envisioned it.

%\section{Spatial Computing}

In this proposal I adopt Greenwold’s original definition of \textit{spatial computing} as ``human interaction with a machine in which the machine retains and manipulates referents to real objects and spaces.'' (This definition also subsumes the more recent uses of spatial computing in marketing material as anything to do with AR/VR/MR.) This aspect of being rooted in the real-world lends spatial computing toward tasks that occur beyond the office desk. For example, the second-generation of Microsoft HoloLens and Magic Leap augmented reality (AR) devices were largely marketed toward industrial applications such as manufacturing line guidance, surgical operations or in-field visualizations. These types of tasks require spatial computing technologies to work in a given \textit{context}, honoring the user’s current workflow or situation. 
 
 
%\section{In-Context Computing}
 
Dey and Abowd use \textit{context} to refer to “any information that can be used to characterize the situation of an entity. An entity is a person, place, or object that is considered relevant to the interaction between a user and an application, including the user and applications themselves.” While this definition is typically leveraged for defining \textit{context-aware} computing -- applications that are actively responsive to changes in context -- in this proposal I instead focus in on what I call \textit{in-context} computing. In-context computing is used to assist the user in an application that exists beyond the computer itself, such as while they performing another task or in a particular environment. For example, in-context computing could be an in-flight computer display meant to relay information to a pilot while flying or a conversational voice assistant for navigational queries as the user is walking. This would be held in contrast to word processing on a laptop computer or a mobile game on a smartphone where the application and user task are one and the same, and the application has no relation to the user’s situation or environment. In-context computing can offer assistive extensions of the human’s capability to allow for multi-tasking or greater efficiency in the task at hand. 
 
 
%\section{Putting It Together}
%\section{Being Spatial and In-Context Computing Together}
 
As they are both tied to the environment and task-at-hand, spatial and in-context computing are two sides of the same coin. \textbf{I posit that by implicitly leveraging the \textit{spatial} context of a user’s environment, technologies can better serve users as they operate within a situation, for example, while participating in another task or operating while mobile.} Through system building, I explore design constraints and considerations that follow from making spatial computing operate within a given situation, environment, or task. For instance, the latency, precision of interaction, form factor, robustness, power, social acceptability, comfort, and intrusiveness are a handful of key constraints that arise when designing an in-context computing system. For a holistic perspective, I approach this topic from three angles: \textit{spatial computing interaction}, \textit{spatial computing sensing}, and \textit{spatial computing control}.

\subsubsection{Spatial computing interaction:} Beginning with spatial computing interaction, the focus is on co-designing spatial input and output mechanisms that align with the user's workflow, enabling seamless support within their work context. Specifically, in Augmented Reality Debugging Workbench (Chapter 3 and 4), we ask: can augmented reality assist electrical engineers in their printed circuit board debugging workflows and how can it best support them? (\textbf{RQ1}) To answer this question, we design and develop an augmented reality workbench for electrical engineers. To facilitate in-context computing, we co-design the interactions around their existing workflows leveraging their workbench surface as a spatial canvas for projection and input.

\subsubsection{Spatial computing sensing:} Moving on to spatial computing sensing, the emphasis lies in understanding and utilizing the spatial characteristics of a given situation to enhance computing performance. Specifically, I focus on spatial audio capture sensing in noisy, real-world environments. In ClearBuds (Chapter 4), we ask: How we can leverage spatial information, in addition to frequency information, about competing environmental sound sources to remove them and allow for clear calls? (\textbf{RQ2}). We develop a pair of wireless earbuds that uses multiple, time-synchronized audio channels and a time-domain network to utilize the spatial distribution of the target speech source and interfering noise sources for speech enhancement.

\subsubsection{Spatial computing control (proposed):} Lastly, attention is given to the control of future spatial computing devices, particularly in mobile scenarios. A challenge lies in designing input modalities for smartglasses that are ergonomic, socially acceptable, and precise, without relying on an external surface. In ToFRing and Placeholder (Chapter 5), we propose to develop two systems to investigate: how can thumb-to-finger microgesture inputs be sensed from a wearable ring and wristband respectively? (\textbf{RQ3}) In ToF Ring, we plan to develop a system that can gather thumb-to-index swipe and thumb motion information in two axes using miniaturized time-of-flight and bio-acoustic sensors. In Placeholder, we plan to leverage electrical sensing to be able to sense thumb-to-index gestures from a wrist-based system, allowing for users who prefer bracelets or watches to similarly be able to control smartglasses. In both systems, the design constraints applied by on-the-go usage influences the interactions and form factors we can consider.

% ========== Chapter 2
 
\chapter{Related Work}
\label{sec:related}

In this section, we review related work for each of the areas of focus: spatial computing interaction (section \ref{sec: Spatial Computing Interaction: Augmented Task Guidance}, sensing, and control. We contextualize the work within the general thesis and then narrow to the specific contributions of the individual works that realize that hypothesis.

\section{Spatial Computing Interaction: Augmented Task Guidance}
\label{sec: Spatial Computing Interaction: Augmented Task Guidance}

\subsection{Augmenting Reality}
\label{subsec: Augmented Reality}

Our own senses are mediated by space -- how we see, how we hear, and how we touch. Through our development, we generate natural intuitions about our body's relation to space [Piaget, J. (1954)]. Therefore tapping into these natural faculties can better lend itself to what Weiser envisioned of computing as "an extension of our unconscious." 
%\subsection{Augmented Visualization and Interaction for Spatially-Associated Information Access}
%\label{subsec: Augmented Visualization and Interaction for Spatially-Associated Information Access}
Augmented reality (AR) has long been seen as a paradigm that can decrease the barrier between virtual and physical information transfer [cite Grasping Reality Through Illusion- Interactive Graphics Serving Science, Annotating the real world with knowledge-based graphics on a see-through head-mounted display.].
% above is the big claim about AR, below is breaking that claim down
This transfer process can consist of two components: the presentation of the information to the user, generally in the form of visualizations; and the ability for the user to interact with the visualization, perhaps enabling the user to query for additional information.
% give examples of the first part of the claim
Prior work has shown that AR systems presenting spatially-tracked information, even with no interaction component, can be effective not only in reducing error rate and mental effort across industrial tasks.
Feiner et al.’s seminal KARMA system \cite{Feiner1993Knowledge-basedReality} sought to provide augmented instruction in support of complex 3D tasks, for example guiding the user through laser printer maintenance with overlaid leader lines, callouts and arrows bound to different components of the machine. 
TODO such as order picking \cite{Schwerdtfeger2008SupportingReality} and object assembly \cite{Caudell1992AugmentedProcesses, Tang2003ComparativeAssembly}.
% then give examples of making them interactive
Extending AR experiences to enable interaction makes them even more powerful.
Such interactions might enable actions such as the selection of elements in the physical world to be used as part of a virtual tool operation.
Digitaldesk \cite{Wellner1993InteractingDigitaldesk} demonstrated such interactions with examples such as allowing the user to move a number from a physical price list into a virtual calculator.
% we work on both of these aspects of AR and we will talk about how previous people have used both of these aspects of AR wrt PCBs
In this first section on spatial computing interaction, our work continues the line of research into helpful augmented reality in industrial contexts. We explore the design space of both of AR visualization and AR interaction, to understand how they might enhance electrical engineers' workflow with PCBs. We observe that their workflow requires frequent context switching between various virtual design files and spatial navigation on their physical design. We take a slight detour from spatial computing technologies to examine their current tools.
% In the following section, we discuss how prior work has used these aspects of AR toward working with PCBs.


% separating line hereeeeeeeeeeee



% Digitaldesk \cite{Wellner1993InteractingDigitaldesk} sought to drive productivity workflows by generating virtual proxies for physical objects by augmenting a desk surface with an overhead camera-projector system. Central to the work’s goal is the movement of data or information between the real and virtual environments, such as transferring a number off a physical price list into a virtual calculator.
% % Ishii \cite{Leithinger2011DirectDisplay} work here?
% In particular, AR has been of great interest for information access that carries a spatial correspondence to objects in reality \cite{Azuma1997AReality, Feiner1993Knowledge-basedReality}, such as order picking \cite{Schwerdtfeger2008SupportingReality} and object assembly \cite{Caudell2003AugmentedProcesses}. Tang et al. \cite{Tang2003ComparativeAssembly} found a decrease in error rate and perceived mental effort in assembly tasks with spatially-tracked AR compared to head up display and instruction manual, however noted that calibration, display, and tracking techniques can have a significant effect on assembly task completion time. The design paradigm of rendering abstract information with corresponding spatially-situated objects has been explored for a long time. Feiner et al.’s KARMA system \cite{Feiner1993Knowledge-basedReality} sought to provide augmented instruction in support of complex 3D tasks, for example guiding the user through laser printer maintenance with overlaid leader lines, callouts and arrows bound to different components of the machine. Instructional augmented overlay has been applied to electronics in a number of instances. Projectron Mapping \cite{Akiyama2014ProjectronMapping} seeks to provide a delightful, tangible feedback using projective AR to light components when a virtual circuit is correctly completed by students. Flow of Electrons \cite{Conradi2011FlowExperientially} helps novices learn about circuit topology and function by allowing physical electronics to be virtually connected with projected augmented overlays on a surface, allowing for experimentation, animated feedback (electron flow through connections), and educational nudges from the tutorial mascot. The study revealed augmentation helped to instruct novice users to build circuits, but the linear arrangement of the tutorial did not allow for a transition to autonomous learning. LightUp \cite{Chan2013LightUp:Electronics}, a block-based teaching tool for children, also utilized mobile AR to help show the movement of electrons through the system. Each of these projects indicate augmented reality can be an enriching tool to bridge the gap between abstract concepts/metadata and real-world circuits. Our work seeks to leverage the design paradigm of providing just-in-time, spatially-situated augmented instruction to lower workload, errors, and workload for PCB debugging.

% \subsection{Augmented Reality}
% \label{subsec: Augmented Reality}

% Augmented reality (AR) involves creating an experience in which a user's sense of reality is altered by superimposing a synthetic image onto the user's view of the real-world environment.
% AR is most commonly implemented using head-mounted displays (HMDs) in order to enable mobility, which allows for always-available functionality.
% As a result, HMD-based AR has primarily found applications in industrial settings, such as order picking and part assembly.
% However, HMD-based AR also introduces many technical complexities, such as making the hardware compact enough to be worn on the head, and designing and calibrating a display that effectively renders the composite image.

% For applications that do not require mobility, AR can alternatively be achieved using a static projector.
% Such a setup is not as constrained in terms of physical space, and simplifies the requirements of the display, since the superimposition of the image occurs physically.
% A common use-case for projected-based AR is the desk or workbench.
% DigitalDesk ..., perceptive workbench...

% Our work describes a hypothetical projected-based AR system for augmenting PCBs... explore the design space for faster interactions and information retrieval.

% \subsection{Visualization Tools for Breadboard Designs}
% \label{subsec: Visualization Tools for Breadboard Designs}



% \subsection{Augmenting Breadboards}
% \label{subsec: Augmenting Breadboards}
% \subsubsection{Visualization tools}
% Because of their easy solderless reconfigurability, breadboards are commonly used amongst hobbyists, students, designers, and engineers for prototyping simple, one-off circuits.
% Fritzing \cite{Knorig2009FritzingDesigners} was designed to support designers and artists in capturing breadboard designs virtually as well as translating breadboard designs into layouts.
% Important to the tool is the “breadboard view” which seeks to emulate the tangible experience of physically building circuits by having draggable wires and drop-in, semi-realistic components.
% Similar to how the layout file is a virtual spatial capture of a PCB, the Fritzing diagram serves as a “digital twin” to a real breadboard circuit, providing a virtual copy on which future tools add augmentation.

% % \subsection{Visualization Tools for Breadboard Designs Incorporating Interactivity and Measurement}
% % \label{subsec: Visualization Tools for Breadboard Designs Incorporating Interactivity and Measurement}
% \subsubsection{Adding interactivity and measurement}
% A number of systems have extended the virtual breadboard visualizations to allow for augmented interaction or measurement capability.
% Proxino \cite{Wu2019Proxino:Proxies} extends Fritzing by allowing for physically maniputable sensor proxies that can drive values in a virtual components within a  Fritzing circuit.
% Visible Breadboard \cite{Ochiai2014VisibleElectricity} allows for wiring to be performed with swipes directly on the board, and the connected nodes are highlighted in the same LED color.
% Toastboard \cite{Drew2016TheCircuits} and CurrentViz \cite{Wu2017CurrentViz:Circuits} collect real-time state information of prototyped breadboard circuits on custom instrumented breadboard hardware, specifically voltage and current information respectively.
% Toastboard pushes voltage information into a virtual diagram, and also provides row-associated status LEDs which were cited by all participants as being useful for debugging feedback. In the CurrentViz, current information from an instrumented breadboard is visualized atop a corresponding Fritzing diagram.

% While measurement points can be arbitrarily accessed in breadboarded setups, access points to pull current information must be designed into PCBs a priori, usually in the form of external sense resistors or internal sense resistors within PMICs (power management integrated circuits) or other ICs.
% RwmLAB \cite{Asumadu2003AInstrument}, Scanalog \cite{Strasnick2017Scanalog:Hardware}, VISIR \cite{Tawfik2013VirtualBreadboard} and VirtualComponent \cite{Kim2019VirtualComponent:Circuits} leverage this arbitrary circuit node access allowed by breadboarding to perform component insertion.
% Each tool has an associated breadboard GUI where users can design a virtual circuit and an instrumented breadboard is re-wired to match this digital twin. 
% Measurements can be taken on the physical circuits via the GUI.
% Scanalog and VirtualComponent both allow access to mix physical and virtual components, and VirtualComponent provides a mobile AR based overlay of the virtual components on a video feed of a physical breadboard.
% Each of these tools, while supporting the maker, student, or DIY hobbyist may not support the needs experience in the electronics industry.
% Breadboards are a quick and easy platform to iterate on the topology and values of simple analog and low-speed digital circuits.
% However in industry, circuits are rarely prototyped on breadboards as the circuits are too complex to hand-assemble into a breadboard format, many ICs are not available in DIP packages compatible with breadboarding, and parasitic capacitance in breadboard construction can affect sensitive or high speed signals. 

% \subsection{Visualization Tools for PCB Designs}
% \label{subsec: Visualization Tools for PCB Designs}
\subsection{Limitations of Current Tools}

During the process of debugging a new PCB design, engineers must constantly move between their schematic, layout, and physical representations in order to validate their design or understand the nature of a design failure\footnote{Some helpful technical background and terminology: The electrical engineering design process typically starts with designing a circuit to meet a set of functional requirements.
Using an electronic computer-aided design (ECAD) tool, the logic of the circuit is formalized via schematic capture into a \textit{schematic diagram}, which visualizes the circuit's components as symbols and the circuit's interconnections (\textit{nets}) as topological lines between the components' pins.
This logic is then transferred to a \textit{layout diagram}, where components and connections are placed in a physical coordinate space.
Finally, the design is then physically fabricated and assembled into a functional PCB, where the components are soldered onto the surface of a fiberglass board with conductive pads, vias, and traces running buried within its layers. For images, see online tutorials on Sparkfun (\url{https://learn.sparkfun.com/tutorials/pcb-basics/all}) or Adafruit (\url{https://learn.adafruit.com/making-pcbs-with-oshpark-and-eagle}).}
(Fig. \ref{fig:teaserfigure}).
Through current ECAD tools, this can be a time-consuming and error-prone process which typically involves manually following correspondences and nomenclatures
% demarcated by a component reference designator (such as ‘{\fontfamily{cmtt}\selectfont{R234}}’), net name (‘{\fontfamily{cmtt}\selectfont{PP\_VSYS\_5V}}’), or approximate location on the board,
across different software applications, often using each application’s textual ``find'' command. To locate the corresponding area-of-interest on the board, the engineer must textually match the reference designator against the board’s silkscreen (if printed) or visually pattern match layout representation to the physical PCB, which can become more challenging with dense components or different orientations.
As a debugging procedure typically involves tens to hundreds of these correspondences, this procedure can quickly become tedious with the ever-increasing complexity of board designs only exacerbating the challenge. 
In our work, we interview electrical engineers on their workflow to get a better picture of pain points in their current processes. We design our spatial computing system with an eye towards relieving these pain points.

\subsection{Augmented Reality for PCBs}

While PCBs are often considered the staple of industry level electronics, breadboards are often used by students and hobbyists for their solderless reconfigurability that enables rapid iteration. However, they are rarely used as part of the hardware development process in industry.
While recent work in the HCI community aimed at the student population has demonstrated a number of breadboard augmentation techniques \cite{Ochiai2014VisibleElectricity, Drew2016TheToastboard, Wu2017CurrentViz, Kim2019VirtualComponent}, PCBs are substantially more intricate, requiring much more careful augmentation.
In this section, we discuss more directly comparable related work in the realm of specifically augmenting PCBs.


\subsubsection{Visualization Tools}
A few tools support visualizing certain component metadata, such as location, directly on the PCB.
InspectAR \cite{InspectARTools} is a recently released tool that uses mobile AR to overlay elements of the layout and associated metadata onto a camera view of the PCB displayed on a mobile tablet or PC.
It is targeted toward supporting industry professionals, with couplings to industry standard ECAD tools.
The tool does not seem to support direct interaction with the PCB itself, measurement interactions, or a topological schematic view.
The sales webpage offers strong testimonials speaking to the increased assembly and debugging efficiency from decreased context-switching, claiming “an average 30\% reduction in lab-time.”
While these indications speak strongly to the hypothesis that mixed reality visualization of layout metadata on PCB can increase efficiency, a systematic study is yet to be published.
The Mascot \cite{MascotRobotas}, a robotic workbench from Robotas, helps to support operators performing hand assembly of through hole components
by steering a projected laser spot to the installation location on an anchored PCB.
%The tool allows for preloading of assembly steps, which controls automated picking carousels and a projected laser spot showing assembly position on an anchored PCB.
Similarly, Hahn et al. \cite{Hahn2015AugmentedProcess} generated an AR tool with textual and graphical cues delivered through a smartglass for assisting workers performing PCB assembly, indicating that the tool allowed for errorless part picking and assembly.
Hahn et al.’s tool, InspectAR and Mascot all provide board-locked augmented instruction for PCB workflow, driving information from the virtual design files to the user’s view of the PCB. Our work broadens the design space seeking to also incorporate augmented interaction and measurement to pass data in the opposite direction, that is, interactive capture in the PCB view can be passed to the virtual design files.

% \subsection{Visualization Tools for PCB Designs Incorporating Interactivity and Measurement}
% \label{subsec: Visualization Tools for PCB Designs Incorporating Interactivity and Measurement}
\subsubsection{Adding Interactivity and Measurement}
Pinpoint \cite{Strasnick2019Pinpoint} is a tool designed to assist in PCB debugging by allowing users to modify and measure the circuit \textit{in situ} after the PCB is fabricated. The tool modifies the layout of a PCB by inserting breakable connections on some traces. While not using augmented reality per se, the tool connects the virtual and the physical by using GUI-controlled relays to make and break these connections. For form factor designs and mass-produced PCBs, modifying the layout for test is typically restrained to adding test points on critical nets for bed-of-nails, on-line testing or manual access for workbench debugging. Our work seeks to support existing debugging workflows that do not modify the PCB design, and instead ease access to measurement points by guiding users with augmentations.

%Pinpoint \cite{Strasnick2019Pinpoint:Instrumentation} extends the concept in Scanalog, VISIR, and Virtual Component to PCB designs. The tool adds up to 16 jumper pads into a PCB design before fabrication, generates a bed-of-nails jig to interface with the jumper pads, and modifies connections through a GUI-controlled relay board. While the tool adds a unique layer of debugging capability akin to breadboarding -- the ability to access and isolate parts of a circuit for easy measurement or modification -- it necessitates amending the original design routing. For form factor designs and mass-produced PCBs, a high level of control in layout is maintained by the designer with board size, manufacturing, cost, and signal integrity concerns often precluding the use of an auto routing mechanism. The modifications to layout in production designs in support of test is therefore typically restrained to adding test points on critical nets for bed-of-nails, on-line testing or manual access for workbench debugging. Our work seeks to support existing debugging workflows that do not modify the PCB design, and instead ease access to measurement points by guiding users with augmentations.
More relevant to our work, BoardLab presents a magnetically tracked stylus that enables interactions from board to schematic, such as selecting and identifying components on the schematic by touching the components on the board as well as taking voltage measurements and having the measurement annotated on the schematic \cite{Goyal2013BoardLab}.
Although the system looks promising, no formal evaluation was reported.
Our work studies whether the interactions afforded by such a stylus would be helpful to electrical engineers as they actively debug, as well as exploring interactions that are synergistically enabled as augmented interaction and measurement is paired with simultaneous augmented visualization.


%Augmented Reality (AR) have the potential to be highly adaptable and customizable to individual users and specific use cases, while minimizing physical barriers between virtual and physical information transfer. %Recent work have demonstrated the feasibility and effectiveness of using AR to display information on PCBs~\cite{Chatterjee2021AugmentedBoards}. Augmented Silkscreen presented a set of augmented reality interaction techniques to assist electrical engineers in PCB debugging. They found that combining augmented visualization and augmented interaction on printed circuit boards unlocks promising avenues to alleviate the frequent context switching~\cite{Chatterjee2021AugmentedBoards}. In the reset of this section, we will first focus on related work on enabling better debugging workflow on breadboards and later we provide literature related to extending the capabilities of PCBs.














\section{Spatial Computing Sensing: Spatial Audio Capture} 

\subsection{Sensing Space and Distance}

Historically, systems have used a wide variety of sensing methodologies to understand space. Radar, sonar, and, more recently, GPS were all initially developed for wartime applications with battlefield-scale positioning. Within the room-scale, Sutherland's pioneering augmented reality system explored the use of multi-frequency continuous-wave ultrasound, though the first system relied on a set of taut lines on reels sensed by positional encoders. The calculated device pose was used to update the see-through graphics in real-time. Modern AR and VR head-mounted devices utilize a passive method of 6DOF positioning, derived from simultaneous localization and mapping (SLAM) robotics. Images from multiple on-headset cameras are processed to generate a 3D visual map of world keypoints. Intermediate positions are calculated by integration of a calibrated inertial measurement unit (IMU) and are fused with vision via an Extended Kalman Filter. This technique has been key to allow for these HMDs to be mobile and used in-the-field without need for external tracking hardware. For dense mapping of the geometry of surrounding environment, a number of headsets employ a time-of-flight depth camera which can be leveraged for semantic understanding of the surrounding geometry, accurate interactive plane detection, and hand tracking. %do i need to add polhemus?

With the exception of SLAM, each of these systems utilize precise timings of reflected or transmitted waves with a known velocity to calculate distance. This same technique is utilized in the audio domain to localize sound sources. Our ears are separated by a known baseline. Audio arriving to our ears from an off-center source arrive at slightly different times resulting in an interaural time difference (ITD) given the speed of sound [lord rayleigh duplex theory]. This phenomenon, along with interaural level differences (ILD) and frequency modulations resulting from the head-related transfer function, are the primary cues that allow humans and other animals to precisely localize sound sources.

In noisy environments, like when on a busy street, multiple competing sound sources layer on top one another creating a noisy mixture. Dubbed the cocktail party effect, our brain's ability to focus auditory attention on a given sound source while filtering out other stimulus has been studied since at least the early 1950s. Interaural time differences [cite binaural unmasking] as well as the audio frequency content both play a part in source separation. Here we focus on a producing a system that can utilize both cues together to enhance a user's voice from amongst a noisy background. We apply this technique specifically to a set of wireless earbuds. Because of their portability, these systems are often used to take calls in noisy, dynamic environments. Real-time speech enhancement is a long standing problem in the signal processing and machine learning communities.  While recent advances in the machine learning community have shown promising results, none of them have been demonstrated on wireless earbuds. Further, the vast majority cannot run on mobile devices and meet these real-time constraints. As a result, endfire beamforming configurations remain popular on most consumer mobile phones and earbuds \cite{samsungglobalnewsroom_2014, airpods, sennheiser_2020, beamforming-app-note}. Below, we briefly discuss beamforming, single channel speech enhancement networks, and binaural networks. By creating a wireless acoustic  network between two earbuds and a novel light-weight hybrid  time-domain and spectrogram-based  neural network, we show for the first time that real-time two-channel neural networks can outperform  current real-time speech enhancement approaches for wireless earbuds.

\subsection{Spatial Audio Capture}

\subsubsection{Beamforming techniques.} A common approach to enhancing speech is to design a beamforming microphone array to be more sensitive to sounds coming from the direction of the user's mouth \cite{van1988beamforming} or voice~\cite{dov-uist21}. Since signal-processing based beamforming is computationally lightweight compared to other speech enhancement techniques, these techniques are deployed on many commercially available audio products today such as smart speakers \cite{amazon}, mobile phones \cite{samsungglobalnewsroom_2014}, and earbud devices like Apple AirPods \cite{airpods}. However, the performance of beamforming is limited by the geometry of the microphones and the distance between them \cite{van1988beamforming, InvenSense}. The form factor of devices like AirPods restricts both the number of microphones on a single earbud and the available distance between them, limiting the gain of the beamformer. While beamforming simultaneously across two earbuds could provide better performance in principle, current wireless architectures are limited to streaming from a single earbud at a time \cite{bluetooth}. Furthermore, adaptive beamformers such as MVDR \cite{frost1972MVDR}, while showing promise with relatively few interfering sources,  are sensitive to sensor placement tolerance and steering \cite{zhang2017deep, brandstein2001microphone}. Finally, beamforming leverages spatial cues only and does not use acoustic cues and perceptual differences to discriminate sources, information that machine learning methods leverage successfully.

% Don H. Johnson and Dan E. Dudgeon. Array Signal Processing: Concepts and Techniques. Simon & Schuster, Inc., USA, 1992

% Beamforming microphone arrays for speech enhancement, https://ieeexplore.ieee.org/abstract/document/225915

% Rate-Constrained Beamforming in Binaural Hearing Aids
% https://link.springer.com/content/pdf/10.1155/2009/257197.pdf

% Dual-Channel Speech Enhancement by
% Superdirective Beamforming
% https://link.springer.com/content/pdf/10.1155/ASP/2006/63297.pdf


%\vskip 0.05in\noindent{\bf Single-channel speech enhancement.} 
\subsubsection{Single-channel speech enhancement}
Recent deep learning techniques have led to significant progress in single channel speech enhancement methods. These models typically operate on spectrograms to separate the human voice from background noise \cite{realtimenoise, Mohammadiha_2013, online_nonnegative, nikzad2020deep, choi2019phaseaware, lstm_speechenhancement, fu2019metricgan, TFMasking}. However, recent trends have opted to operate directly on the time domain signals \cite{luo2019conv, germain2018speech, pascual2017segan, demucsreal, macartney2018improved}, yielding performance improvements over spectrogram approaches. Commercially available noise suppression software like Krisp \cite{krisp} and Google Meet \cite{googlemeet} have successfully deployed single-channel models in real-time and are available for use on mobile phones and desktop computers, processing is performed on the cloud. However, single-channel models can not effectively capture the spatial information and hence fail to isolate the intended speaker when there are multiple speakers present.

% --> some recent work on increasing performance to real-time applications
% Real Time Speech Enhancement in the Waveform Domain
% https://arxiv.org/abs/2006.12847

\subsubsection{Multi-channel source separation and speech enhancement}
%\subsection{Multi-channel source separation and speech enhancement}
Multi-channel methods have been shown to perform better than their single-channel source separation counterparts \cite{yoshioka2018multi, chen2018multi, zhang2017deep, gu2020enhancing, tzirakis2021multichannel, jenrungrot2020cone}. Binaural methods have also been used for source separation \cite{binaural1, han2020realtime, li2011two, reindl2010speech} and localization \cite{van2008binaural, lyon1983computational, kock1950binaural}. Our method improves on existing binaural methods by combining time-domain neural network with spectrogram-based frequency masking networks as well as optimizing them to enable   real-time processing on a phone. Recent works~\cite{binaural_osu, dual_phone} use multiple microphones on a smartphone for  speech-enhancement however neither of them demonstrates real-time performance of a smartphone. In contrast, we demonstrate the first system that achieves real-time speech enhancement using microphones on the two wireless earbuds. Further, as the distance between the earbuds is larger than the distance between microphones on a typical mobile phone, we can  attain a better baseline than a mobile phone implementation, while also retaining the ability to speak hands-free.

% \subsubsection{Earable computing and platforms}
% There has been recent interest in earable computing and sensing~\cite{oesense21,esense-1,esense-2,plat-1,romit-1}. Earable platforms such as eSense~\cite{esense-1,esense-2} have enabled research in enabling various sensing applications using earable devices. eSense however does not support time-synchronized audio transmission from two earbuds to a mobile device. This is a critical requirement for achieving  speech enhancement using the  microphones on the two wireless earbuds.  OpenMHA~\cite{open-1,open-2} is a open signal processing {\it software} platform for hearing aid research. In contrast, we create wireless earbud hardware, which will be open-sourced at publication and can support synchronize wireless transmission from the two earbuds to achieve speech enhancement. 

% --> multi-mic
% Cone-of-silence

%Some approaches, such as \cite{binaural_osu, dual_phone} have used the multiple microphones on a smartphone for real-time speech-enhancement. However, these networks would have to be retrained for every mobile phone as the position and distance between these microphones could change with future iterations. As our method is instead deployed on wireless earbuds, the geometry of the speaker in respect to the microphones will remain consistent. Further, as the distance between the earbuds is larger than the distance between microphones on a typical mobile phone, we attain a better baseline than a mobile phone implementation, while also retaining the ability to speak hands-free.











 

\section{Spatial Computing Control: Microgesture Interaction}

As screens have shrunk from desktop computer to mobile phone to smartwatch, the friction to accessing screen content has decreased, but the throughput of delivered content has suffered. To overcome these limitations, there has a huge amount of investment towards consolidation of display and sensing into a head-mounted device (HMD) that can deliver information across the user's field-of-view (FOV)%anywhere.
A variety of flavors of HMD have been developed to tackle both sides of the spectrum -- to decrease information friction as well as to deliver greater immersion and throughput.
%Barring engineering limitations, an HMD allows the display space to one day be infinitely large as the user's field of view.
Smartglasses can operate as a head-up display for timely and spatially relevant digital content. Higher-end mixed reality devices can maintain spatial coherence of the augmentations during movement, providing the illusion of world-locked content. Virtual reality devices can provide a sense immersion that transports the user to a new environment.

To be able to control these immersive devices, 6DOF controllers are typically employed. These controllers use outside-in or inside-out camera-based tracking. Outside-in methods involve a set of cameras on the headset tracking a constellation of infrared LEDs on the controller. Two limitations are that the controllers must be within the headset camera's FoV and have a ring of LEDs positioned in a way that the constellation is not obstructed. Inside-out tracking leverages multiple cameras within the controller with the same SLAM algorithm discussed in the previous section. With the additional compute and cameras this technique tends to be higher power and require a few seconds upon start up for map relocalization. The controllers offer compelling experiences for high-fidelity, immersive gaming, where low-latency hand positioning and the gamepad controls can provide precise manipulation and action interactions. However, as we think toward in-context spatial computing where users are performing another task like maintenance, surgery, or typing, it is essential to keep the hands unencumbered.

Indeed, augmented and mixed reality devices also offer articulated hand tracking using computer vision on depth camera or stereo camera data \cite{microsoft, MetaStore, TrackingUltraleap}. This allows for direct manipulation of buttons and content similar to the [egocentric?, ergotic?] interaction techniques studied with data gloves in the early day of VR [A hand gesture interface device zimmerman]. While hand tracking now does not require donning a glove, it does require the user to keep their hands within the field-of-view of the camera. Therefore, this method of control, as well as those involve keeping a controller within camera FoV, suffer from ``gorilla arm syndrome.'' As we consider augmented reality that will be used in more public settings such as smartglasses, large motions can draw unwanted attention to the user posing social acceptability challenges, such in meetings, walking down the street, or on a subway [Usable gestures for mobile interfaces: evaluating social acceptability]. Furthermore, the privacy implication of having an always-on, on-headset cameras will also need to be evaluated in public and private scenarios. These challenges motivate two aspects required for public in-context spatial computing: (1) the need for the interaction to be suitably discreet, accessible, and quick enough for on-the-go use and (2) the need for such interactions to be sensed without line of sight from a headset. This has motivated explorations of microgestures from body-worn sensing platforms.   %Smartglasses systems also need to be lighter and lower-power and camera-based solutions can take hundreds of milliwatts for both sensing and processing.

% Direct manipulation
% Indirect manipulation (ray casting etc.)
% Decoupled 


\subsection{Microgestures in Context}

Ashbrook defines microinteractions as device interaction under four seconds, citing Oulasvirta's investigation into user attention during mobile situations. While engaging in activities like navigation, eating and conversing, participants shifted their attention for four to eight seconds to the device before returning to the primary task. This means the friction to providing input while mobile must be even less, prompting Ashbrook to explore touch gestures and bezel manipulation on wristband-based devices. Similarly, many systems have explored sensors mounted on the wrist, hand, and/or fingers to sense the more subtle motion of these appendages without limitations on headset field of view. These systems span a wide range of modalities: {optical (\cite{Chatterjee2016TouchPoint:Device}, \cite{Kim2012Digits}, \cite{GestureWristRekimotoTODO get this},
\cite{McIntosh2017SensIR:Reflection}, \cite{Chan2015Cyclopsring:Ring},  \cite{loclair2010pinchwatch}), (\cite{Gong2017Pyro:Sensing}, \cite{thermalring}), bio-acoustic (\cite{10.1145/506443.506566}, \cite{sot}, \cite{fingerping}), 
ultrasonic (\cite{Iravantchi2019BeamBand:Beamforming}, \cite{Zhang2017SoundTrak}), mechanical (\cite{Takada2019AFiber}, \cite{Lin2015BackHand:Hand}, \cite{10.5555/3298830.3298870}), electromyography (\cite{saponas enabling always avialabe input with mci}),
and inertial (\cite{Guy}, \cite{Meier2021TaplD:Sensing}, \cite{Fukumoto1994FingeRing:Interface}, \cite{Liang2021DualRing}).}
The capability of these devices ranges from detecting pinches to recognizing discrete gesture sets to driving full kinematic models of the hand. A shared design goal of each of these systems is to keep the hand itself unencumbered allowing for the user's primary task to be minimally affected, so called free-hand or device-free gesture systems.

\subsection{Thumb-to-Index Microgestures}

Amongst these systems, we focus in on devices that enable single-handed, thumb-to-index microgestures.  Neurophysiological findings indicate that fingers have some of the largest representations in both the somatosenosry and motor cortexes as function of their size. The fingertips feature both a high spatial resolution of tactile sensing as well as highly dexterous control. Zhai demonstrated that leveraging small muscle groups like fingers demonstrated manipulation performance that exceeds input relying just on wrist, arm, and shoulder [influence of sucle group on perofrmanf oc mulitple degree of feedom input]. Amongst the fingers, user elicitation studies prompting users to suggest hand gestures for common digital functions resulted in most gestures being executed between the thumb and index fingers [chan user elicitation]. This tracks with  anatomical analyses indicating the greatest dexterity is between these two fingers [Taxonomy of microinteractions: defining microgestures based on ergonomic and scenario-dependent requirements]. Similary, DigitSpace compared comfort for thumb gestures across different fingerpad areas and found the tip of the index fingerpad to be the highest rated for both taps and swipe gestures. Finally, the propriocepiton between thumb and index finger allowed for a better understanding of the moment of touchdown betwwen the two fingers, enabling higher time doamin accuracy in selection that controller or non-pinch microgestures [Finger Contact in Gesture Interaction Improves Time-domain Input Accuracy in HMD-based Augmented Reality]. These aspects lend thumb-to-index microgestures well to smartglass control where the goal is for a subtle and precise input. Smartglasses differ from other mixed reality headsets, such as Microsoft HoloLens, Magic Leap One, or Meta Quest Pro, in that they are used for daily wear and generally only feature 2D user interfaces for features like notifications and navigation via a monocular display [north][tcl?][oppo?]. Therefore, the input solution must also be similarly appropriate for daily wear and be able to navigate a 2D UI even while on-the-go. In this project, we therefore set a few design goals for a realized system: (1) a sensing suite that is appropriately miniature enough for daily wear, (2) enables ergonomic thumb-to-index microgestures interaction in two dimensions, and (3) robust enough to work across situations and people with no or minimal calibration. 


FingerPad used magnets on the thumb and a Hall sensor grid on the back of the index finger to realize a touchpad on the index surface. However this system requires two components and instruments the fingernail, which is not socially acceptable. 

%Commercial hand gesture detection systems \cite{microsoft, MetaStore, TrackingUltraleap} often rely on optical methods, using cameras mounted on external devices such as AR/VR headsets or necklaces. However, these systems require a clear line of sight to the hand, complicating efforts to  detect gestures made outside the camera's field of view.
% Therefore, a number of systems have explored sensors mounted on the wrist, hand, and/or fingers, spanning a wide range of modalities: \red{optical (Digits \cite{Kim2012Digits},
% TouchPoint \cite{Chatterjee2016TouchPoint:Device},
% SensIR \cite{McIntosh2017SensIR:Reflection}, CyclopsRing \cite{Chan2015Cyclopsring:Ring}, PinchWatch \cite{loclair2010pinchwatch}), thermal (Pyro \cite{Gong2017Pyro:Sensing}, ThermalRing \cite{thermalring}), bio-acoustic (Amento et al. \cite{10.1145/506443.506566}, 
% Mujibiya et al. \cite{sot}, 
% FingerPing \cite{fingerping}), 
% ultrasonic (Beamband \cite{Iravantchi2019BeamBand:Beamforming}, SoundTrak \cite{Zhang2017SoundTrak}), mechanical (DataGlove \cite{Takada2019AFiber}, BackHand \cite{Lin2015BackHand:Hand}, 
% Kuno et al. \cite{10.5555/3298830.3298870}), 
% and inertial (Gu et al. \cite{Guy}, TapID \cite{Meier2021TaplD:Sensing}, FingeRing \cite{Fukumoto1994FingeRing:Interface}, DualRing \cite{Liang2021DualRing}).}





%However, these approaches have several limitations: (1) passive IMUs and bio-acoustic techniques can detect when the fingers make contact  but not when they release, which is important for actions like dragging and dropping; (2) IMUs used for gesture sensing require multiple points of instrumentation, making the system awkward to use; (3) optical and ultrasonic techniques require a clear line of sight to each gesturing appendage, limiting possible mounting positions and making it difficult to detect pinches; and (4) mechanical and magnetic systems require instrumentation of the whole hand, back of the hand, or fingertips, making the system uncomfortable to use.
%The limitations of these objects can be roughly categorized into a few areas: (1) passive IMU and bio-acoustic techniques can detect pinch touchdowns, however cannot detect when the fingers release, an important factor as a clutching mechanism, for example, drag-and-drop, (2) when used for gesture sensing, IMUs require multiple instrumentation points yielding an awkward system, (3) optical and ultrasonic techniques require line-of-sight to each of the gesturing appendages resulting non-ideal mounting positions to capture the full gesture set and are challenged to reliably detect pinches, (4) mechanical and magnetic systems require instrumenting the whole hand, back-of-the-hand, or fingertips, yielding an uncomfortable system.
%In contrast, Z-Ring instruments the body at only a single location, i.e., the base of the index finger, an ergonomic and socially acceptable area for worn systems. {By using the body as a transmission medium, Z-Ring does not require line-of-sight for microgesture sensing. In addition, it can robustly sense both touchdown and touch up events for one- and two-handed gestures even if touch velocity is low.}
%This motivated other optical methods to  explore mounting on the arm or finger to detect gestures. Digits derives an articulated hand model leveraging a wirst-mounted IR camera and projector \cite{Kim2012Digits}. TouchPoint uses a pair of line sensors for continuous finger tracking on the back-of-the-hand \cite{Chatterjee2016TouchPoint:Device}. Back-Hand-See . CyclopsRing enables gesture sensing an \cite{}.
 
% ========== Chapter 3
 
\chapter{Designing AR Debugging Workbench}

% ========== Chapter 4

\chapter{Building AR Debugging Workbench}

% ========== Chapter 5

\chapter{ClearBuds}

% ========== Chapter 6

\chapter{[Proposed] OptoAcoustic Ring}

Starting with the smartphone, we increasingly interact with our computing devices on-the-go, a trend that shall continue as wearables including earbuds, smartwatches and smartglasses become more sophisticated and more integrated with our daily lives. Microgestures, or subtle motions of the fingertips, have been proposed as a compelling method of controlling these devices due to their low fatigue, high precision, social discreetness, and constant availability [cite].
%Thumb-to-index finger microgestures offer an ergonomic, subtle, and always-available method of providing gestural input.
Despite their subtlety, to operate the UIs associated with these devices, the fidelity of interaction afforded by microgestures should match the traditional display and touchscreen affordances on these devices including scrolling/panning in both dimensions, gestures like swipes, and buttons.

We propose OptoAcoustic Ring, a low-profile ring device that uses a miniature sparse, time-of-flight sensor or optical flow sensor and a skin-contact microphone to enable an interactive area on the tip of the index finger. These modalities to be inherently complementary with the time-of-flight allows for finger localization in $x$ and $y$ while the contact microphone can be processed to provide touchdown and swipe events.  

In OptoAcoustic Ring we aim to offer a set of unique features that surpass previous systems in terms of practicality. 
First, with OptoAcoustic Ring we aim to enable a variety of  thumb-to-index-finger microgestures. While previous ring-based system detect unistroke gestures such as swipes and circles, our look to make a system that  enables continuous 1D input in \textit{both} $x$ and $y$ dimensions as well as buttons.
Second, we hope to implement each of these microgesture interactions  in a user-independent manner ensuring the device can be worn and used without calibration.
%In addition to the interaction being unobtrusive, the sensing methodology must  unobstrusive.
Third, we will design OptoAcoustic Ring to sit at the base of the finger similar to conventional jewellery. This positioning allows OptoAcoustic Ring's sensing stack to maintain a low profile unlike other optical-based systems where the optical sensor protrudes from the device to maintain line-of-sight  [cite] or capacitive methods which require obtrusive instrumentation of the fingertip [bleh].
Together these features can help to enable OptoAcoustic Ring to function in an in-context manner to control a pair of smartglasses.

To date, we have built a number of hardware prototypes using  miniature time-of-flight sensors. These sensors are a just several millimeters in each dimension, and provide 1-by-1 up to 8-by-8 pixels of depth data via direct ToF sensing.
After prototyping various configurations of one to four sensors and of different types, we have arrived at a hardware configuration of a single ST VL53L7CX sitting at a 15-degree agree angle at the palm side of the base of the index finger. The angle is slight enough that the sensor remains relatively low-profile. We read a 90deg, 4-by-4 depth image of the shaft of the thumb from the perspective of the index finger at 30 Hz. Because of the depth image's low resolution, the sensor is inherently privacy preserving. However, this also makes it challenging to recover precisely what sensor is seeing, since there are no defining edges to distinguish. Therefore, we employ heuristics across the entire image to predict the x and y positions of the thumb tip on the first fingerpad of the index. First the depth data is inverted in direction and rescaled via a normal curve. This is so that we can easily threshold both far away pixels and noisy pixels together (after inversion and rescaling both tend to be close to zero) and also to provide additional resolution in the range of values associated with the finger depth. X is calculated by a mean of the rescaled values. Y is calculated by taking the weighted linear regression across all points. 

On the bio-acoustic sensing side, we aim to produce a classifier or heuristic that we can use to detect if the fingers are rubbing or tapping. We can use this to then accumulate gesture data from ToF 

We started prototyping with an IMU, however was unable to get good differentiation results using an ML classifier between typing, using the phone, rubbing hands, null, clapping, and our activity of interest -- rubbing fingertips. We think this is becuase we lack distinguishing information avialbe int he higher frequnecies. Instead, we are now using a much higher bandwidth contact microphone. We are still in the process of testing this sensor to understand whether it will give us better results.

A limitation of our approach are the selected optical sensor is approximately 200--300 mW in continuous ranging mode, which is not very low power. However the sensor is designed to achieve  3.5m of ranging, so the transmit power is much greater than our required use case where the subject is 80 times closer. For the same target illuminance, the illumination power increases with the square of the distance. A customized sensor could allow for lower power without significant modification. Additionally the sensor could be turned on opportunistically when the bio-acoustic sensor sensor above a certain power threshold. Because we do not make strong use of the absolute position data returned the the ToF sensor, we will also consider optical flow sensors which can be used in extremely low power methods to detect whether the finger is approaching the sensor, moving away, moving up or down with respect to the sensor.

An additional limitation is that our prototype system thus far is a tethered  device. While we make effort to reduce the size of the sensing system itself to something that is low-profile, we are not attempting to miniaturize the signal processing electronics and will leave that to future work. 

We are currently . A risk to our project is 22







% ========== Chapter 6

\chapter{Plan to Completion and Conclusion}

% ========== Chapter 7

\chapter{Conclusion}

Goals, design motivations

%\printendnotes
%
% ==========   Bibliography
%
\nocite{*}   % include everything in the uwthesis.bib file
\bibliographystyle{plain}
\bibliography{uwthesis}
%
% ==========   Appendices
%
\appendix
\raggedbottom\sloppy
 
% ========== Appendix A
 
\chapter{Where to find the files}
 


\end{document}
