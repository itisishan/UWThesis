%  ========================================================================
%  Copyright (c) 1985 The University of Washington
%
%  Licensed under the Apache License, Version 2.0 (the "License");
%  you may not use this file except in compliance with the License.
%  You may obtain a copy of the License at
%
%      http://www.apache.org/licenses/LICENSE-2.0
%
%  Unless required by applicable law or agreed to in writing, software
%  distributed under the License is distributed on an "AS IS" BASIS,
%  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
%  See the License for the specific language governing permissions and
%  limitations under the License.
%  ========================================================================
%

% Documentation for University of Washington thesis LaTeX document class
% by Jim Fox
% fox@washington.edu
%
%    Revised 2020/02/24, added \caption()[]{} option.  No ToC.
%
%    Revised for version 2015/03/03 of uwthesis.cls
%    Revised, 2016/11/22, for cleanup of sample copyright and title pages
%
%    This document is contained in a single file ONLY because
%    I wanted to be able to distribute it easily.  A real thesis ought
%    to be contained on many files (e.g., one for each chapter, at least).
%
%    To help you identify the files and sections in this large file
%    I use the string '==========' to identify new files.
%
%    To help you ignore the unusual things I do with this sample document
%    I try to use the notation
%       
%    % --- sample stuff only -----
%    special stuff for my document, but you don't need it in your thesis
%    % --- end-of-sample-stuff ---


%    Printed in twoside style now that that's allowed
%
 
\documentclass [11pt, proquest] {uwthesis}[2020/02/24]
 
%
% The following line would print the thesis in a postscript font 

% \usepackage{natbib}
% \def\bibpreamble{\protect\addcontentsline{toc}{chapter}{Bibliography}}

\setcounter{tocdepth}{1}  % Print the chapter and sections to the toc
 

% ==========   Local defs and mods
%

% --- sample stuff only -----
% These format the sample code in this document

\usepackage{alltt}  % 
\newenvironment{demo}
  {\begin{alltt}\leftskip3em
     \def\\{\ttfamily\char`\\}%
     \def\{{\ttfamily\char`\{}%
     \def\}{\ttfamily\char`\}}}
  {\end{alltt}}
 
% metafont font.  If logo not available, use the second form
%
% \font\mffont=logosl10 scaled\magstep1
\let\mffont=\sf
% --- end-of-sample-stuff ---
 



\begin{document}
 
% ==========   Preliminary pages
%
% ( revised 2012 for electronic submission )
%

\prelimpages
 
%
% ----- copyright and title pages
%
\Title{In-Context, Spatial Computing}
\Author{Ishan Chatterjee}
\Year{2023}
\Program{Computer Science and Engineering}

\Chair{Shwetak Patel}{Professor}{Computer Science \& Engineering}
\Signature{Vikram Iyer}
\Signature{Steve Seitz}
\Signature{Jacob Wobbrock}


\titlepage  

\copyrightpage

 
%
% ----- signature and quoteslip are gone
%

%
% ----- abstract
%


\setcounter{page}{-1}
\abstract{%
The desktop computing paradigm has revolutionized the way we live, work, and communicate, but is limited by the physical boundaries of the devices themselves. More recently, mobile computing has allowed us to take these capabilities on-the-go, but input and output mechanisms remain tied to the device surface, without interplay with the physicality of the surrounding environment. Spatial computing, however, seeks to define a new generation of computing, where users can receive digital guidance, communication, and contextual references based on and embedded within the environment around them. This means spatial computing will be used \textit{in-context}, where the users are performing another task — one in which computing can either actively or passively provide support. This provides the new challenge in which the interaction and input of these systems must function within an ongoing situation, but also provides an opportunity where systems can leverage the spatial dimension for additional information to drive natural interaction. I approach this area on three fronts: \textit{spatial computing interaction}, \textit{spatial computing sensing}, and \textit{spatial computing control}.

From the perspective of spatial computing interaction, we investigate how electrical engineers can benefit from spatial computing while debugging printed circuit boards and develop an AR system that supports them in-context. For spatial computing sensing, we design earbuds that filter out the noises in the users’ noisy environment by leveraging the spatial information of interfering audio sources. Finally, for spatial computing control, we develop thumb-to-index finger microgesture interfaces which can allow for control of an augmented reality headset while maintaining the subtlety and precision needed for on-the-go and public scenarios.
}
 
%
% ----- contents & etc.
%
\tableofcontents
\listoffigures
%\listoftables  % I have no tables
 
%
% ----- glossary 
%
% \chapter*{Glossary}      % starred form omits the `chapter x'
% \addcontentsline{toc}{chapter}{Glossary}
% \thispagestyle{plain}
% %
% \begin{glossary}
% \item[argument] replacement text which customizes a \LaTeX\ macro for
% each particular usage.
% \item[back-up] a copy of a file to be used when catastrophe strikes
% the original.  People who make no back-ups deserve
% no sympathy.
% \item[control sequence] the normal form of a command to \LaTeX.
% \item[delimiter] something, often a character, that indicates
% the beginning and ending of an argument.
% More generally, a delimiter is a field separator.
% \item[document class] a file of macros that tailors \LaTeX\ for
% a particular document.  The macros described by this thesis
% constitute a document class.
% \item[document option] a macro or file of macros
% that further modifies \LaTeX\ for
% a particular document.  The option {\tt[chapternotes]}
% constitutes a document option.
% \item[figure] illustrated material, including graphs,
% diagrams, drawings and photographs.
% \item[font] a character set (the alphabet plus digits
% and special symbols) of a particular size and style.  A couple of fonts
% used in this thesis are twelve point roman and {\sl twelve point roman
% slanted}.
% \item[footnote] a note placed at the bottom of a page, end of a chapter,
% or end of a thesis that comments on or cites a reference
% for a designated part of the text.
% \item[formatter] (as opposed to a word-processor) arranges printed
% material according to instructions embedded in the text.
% A word-processor, on the other hand, is normally controlled
% by keyboard strokes that move text about on a display.
% \item[\LaTeX] simply the ultimate in computerized typesetting.
% \item[macro]  a complex control sequence composed of 
% other control sequences.
% \item[pica] an archaic unit of length.  One pica is twelve points and
% six picas is about an inch.
% \item[point] a unit of length.  72.27 points equals one inch.
% \item[roman]  a conventional printing typestyle using serifs.
% the decorations on the ends of letter strokes.
% This thesis is set in roman type.
% \item[rule] a straight printed line; e.g., \hrulefill.
% \item[serif] the decoration at the ends of letter strokes.
% \item[table] information placed in a columnar arrangement.
% \item[thesis] either a master's thesis or a doctoral dissertation.
% This document also refers to itself as a thesis, although it
% really is not one.
 
% \end{glossary}
 
%
% ----- acknowledgments
%
% \acknowledgments{% \vskip2pc
%   % {\narrower\noindent
%   The author wishes to express sincere appreciation to
%   University of Washington, where he has had the opportunity
%   to work with the \TeX\ formatting system,
%   and to the author of \TeX, Donald Knuth, {\it il miglior fabbro}.
%   % \par}
% }

%
% ----- dedication
%
%\dedication{\begin{center}to my dear wife, Joanna\end{center}}

%
% end of the preliminary pages
 
 
 
%
% ==========      Text pages
%

\textpages
 
% ========== Chapter 1
 
\chapter {Introduction}

\section{Spatial Computing}
 
Computing as we know it today is a physically confined experience, most commonly expressed as a box on one's desk or a brick within one's pocket. Interaction with these devices is tied to the device themselves without reference to the user’s environment. The spatial computing paradigm seeks to enable aspects of input and output that move beyond the borders of the physical device, sensing or augmenting aspects of the surrounding physical world. In this proposal I adopt Greenwold’s original definition of \textit{spatial computing} as a “as human interaction with a machine in which the machine retains and manipulates referents to real objects and spaces.” (This definition also subsumes the more recent uses of spatial computing in marketing material as anything to do with AR/VR/MR.) This aspect of being rooted in the real-world lends spatial computing toward tasks that occur beyond the office desk. For example, the second-generation of Microsoft HoloLens and Magic Leap augmented reality (AR) devices were largely marketed toward industrial applications such as manufacturing line guidance, surgical operations or in-field visualizations. These types of tasks require spatial computing technologies to work \textit{in-context} – as an extension of the user’s current workflow or situation. 
 
 
\section{In-Context Computing}
 
Dey and Abowd use \textit{context} to refer to “any information that can be used to characterize the situation of an entity. An entity is a person, place, or object that is considered relevant to the interaction between a user and an application, including the user and applications themselves.” While this definition is then leveraged for defining \textit{context-aware} computing, applications that are actively responsive to changes in context, in this proposal I instead focus in on what I call \textit{in-context} computing. In-context computing is used to assist the user in an application that exists beyond the computer itself, such as while they performing another task or in a particular environment. For example, in-context computing could be an in-flight computer display meant to relay information to a pilot while flying or a conversational voice assistant for navigation queries as the user is walking. (This would be in contrast to word processing on a laptop computer or a mobile game on a smartphone where the application and user task are one and the same, and the application has no relation to the user’s situation or environment.) In-context computing can offer assistive extensions of the human’s capability to allow for multi-tasking or greater efficiency in the task at hand. 
 
 
\section{Putting It Together}
 
As they are both tied to the environment and task-at-hand, spatial and in-context computing are two sides of the same coin. I posit that by implicitly leveraging the \textit{spatial} context of a user’s environment, technologies can better serve users as they operate within a situation, for example, while participating in another task or operating while mobile. Through system building, I explore design constraints and considerations that follow from making spatial computing operate within a given situation, environment, or task. For instance, the latency, precision of interaction, form factor, robustness, power, social acceptability, comfort, and intrusiveness are a handful of key constraints that arise when designing an in-context computing system. For a holistic perspective, I approach this issue from three angles: \textit{spatial computing interaction}, \textit{spatial computing sensing}, and \textit{spatial computing control}.

\subsubsection{Spatial computing interaction:} Beginning with spatial computing interaction, the focus is on co-designing spatial input and output mechanisms that align with the user's workflow, enabling seamless support within their work context. Specifically, in Augmented Reality Debugging Workbench (Chapter 3), we ask: can augmented reality assist electrical engineers in their printed circuit board debugging workflows and how can it best support them? (\textbf{RQ1}) To answer this question, we design and develop an augmented reality workbench for electrical engineers. To facilitate in-context computing, we co-design the interactions around their existing workflows leveraging their workbench surface as a spatial canvas for projection and input.

\subsubsection{Spatial computing sensing:} Moving on to spatial computing sensing, the emphasis lies in understanding and utilizing the spatial characteristics of a given situation to enhance computing performance. Specifically, I focus on spatial audio sensing in noisy, real-world environments. In ClearBuds (Chapter 4), we ask: How we can leverage spatial information, in addition to frequency information, about competing environmental sound sources to remove them and allow for clear calls? (\textbf{RQ2}). We develop a pair of wireless earbuds that uses multiple, time-synchronized audio channels and a time-domain network to utilize the spatial distribution of the target speech source and interfering noise sources for speech enhancement.

\subsubsection{Spatial computing control (proposed):} Lastly, attention is given to the control of future spatial computing devices, particularly in mobile scenarios. A challenge lies in designing input modalities for smartglasses that are ergonomic, socially acceptable, and precise, without relying on an external surface. In ToFRing and Placeholder (Chapter 5), we propose to develop two systems to investigate: how can thumb-to-finger microgesture inputs be sensed from a wearable ring and wristband respectively? (\textbf{RQ3}) In ToF Ring, we plan to develop a system that can gather thumb-to-index swipe and thumb motion information in two axes using miniaturized time-of-flight and bio-acoustic sensors. In Placeholder, we plan to leverage electrical sensing to be able to sense thumb-to-index gestures from a wrist-based system, allowing for users who prefer bracelets or watches to similarly be able to control smartglasses. In both systems, the design constraints applied by on-the-go usage influences the interactions and form factors we can consider.

% ========== Chapter 2
 
\chapter{Related Work}
\label{sec:related}

Human exist as beings in space. Therefore it is intuitive.
We move through space. Our own senses our mediated by space  -- what we see, what we hear, and what we touch. Therefore tapping into these natural faculties can better lend itself to what Mark Weiser calls computing as "an extention of the unconcious"

\section{Spatial Computing Interaction: Augmented Task Guidance}

\subsection{Augmented Reality}%\subsection{Augmented Visualization and Interaction for Spatially-Associated Information Access}
\label{subsec: Augmented Reality}%\label{subsec: Augmented Visualization and Interaction for Spatially-Associated Information Access}
Augmented reality (AR) has long been seen as a paradigm that can decrease the barrier between virtual and physical information transfer.
% above is the big claim about AR, below is breaking that claim down
This transfer process can consist of two components: the presentation of the information to the user, generally in the form of visualizations; and the ability for the user to interact with the visualization, perhaps enabling the user to query for additional information.
% give examples of the first part of the claim
Prior work has shown that AR systems presenting spatially-tracked information, even with no interaction component, can be effective not only in reducing error rate and mental effort across industrial tasks such as order picking \cite{Schwerdtfeger2008SupportingReality} and object assembly \cite{Caudell1992AugmentedProcesses, Tang2003ComparativeAssembly}, but also as a medium for understanding abstract concepts such as how electrons flow through a circuit \cite{Conradi2011FlowElectrons, Chan2013LightUp}.
% then give examples of making them interactive
Extending AR experiences to enable interaction makes them even more powerful.
Such interactions might enable actions such as the selection of elements in the physical world to be used as part of a virtual tool operation.
Digitaldesk \cite{Wellner1993InteractingDigitaldesk} demonstrated such interactions with examples such as allowing the user to move a number from a physical price list into a virtual calculator.
% we work on both of these aspects of AR and we will talk about how previous people have used both of these aspects of AR wrt PCBs
Our work explores the design space of both of these aspects of AR -- visualization and interaction -- and how they might enhance how electrical engineers work with PCBs.
% In the following section, we discuss how prior work has used these aspects of AR toward working with PCBs.


% separating line hereeeeeeeeeeee



% Digitaldesk \cite{Wellner1993InteractingDigitaldesk} sought to drive productivity workflows by generating virtual proxies for physical objects by augmenting a desk surface with an overhead camera-projector system. Central to the work’s goal is the movement of data or information between the real and virtual environments, such as transferring a number off a physical price list into a virtual calculator.
% % Ishii \cite{Leithinger2011DirectDisplay} work here?
% In particular, AR has been of great interest for information access that carries a spatial correspondence to objects in reality \cite{Azuma1997AReality, Feiner1993Knowledge-basedReality}, such as order picking \cite{Schwerdtfeger2008SupportingReality} and object assembly \cite{Caudell2003AugmentedProcesses}. Tang et al. \cite{Tang2003ComparativeAssembly} found a decrease in error rate and perceived mental effort in assembly tasks with spatially-tracked AR compared to head up display and instruction manual, however noted that calibration, display, and tracking techniques can have a significant effect on assembly task completion time. The design paradigm of rendering abstract information with corresponding spatially-situated objects has been explored for a long time. Feiner et al.’s KARMA system \cite{Feiner1993Knowledge-basedReality} sought to provide augmented instruction in support of complex 3D tasks, for example guiding the user through laser printer maintenance with overlaid leader lines, callouts and arrows bound to different components of the machine. Instructional augmented overlay has been applied to electronics in a number of instances. Projectron Mapping \cite{Akiyama2014ProjectronMapping} seeks to provide a delightful, tangible feedback using projective AR to light components when a virtual circuit is correctly completed by students. Flow of Electrons \cite{Conradi2011FlowExperientially} helps novices learn about circuit topology and function by allowing physical electronics to be virtually connected with projected augmented overlays on a surface, allowing for experimentation, animated feedback (electron flow through connections), and educational nudges from the tutorial mascot. The study revealed augmentation helped to instruct novice users to build circuits, but the linear arrangement of the tutorial did not allow for a transition to autonomous learning. LightUp \cite{Chan2013LightUp:Electronics}, a block-based teaching tool for children, also utilized mobile AR to help show the movement of electrons through the system. Each of these projects indicate augmented reality can be an enriching tool to bridge the gap between abstract concepts/metadata and real-world circuits. Our work seeks to leverage the design paradigm of providing just-in-time, spatially-situated augmented instruction to lower workload, errors, and workload for PCB debugging.

% \subsection{Augmented Reality}
% \label{subsec: Augmented Reality}

% Augmented reality (AR) involves creating an experience in which a user's sense of reality is altered by superimposing a synthetic image onto the user's view of the real-world environment.
% AR is most commonly implemented using head-mounted displays (HMDs) in order to enable mobility, which allows for always-available functionality.
% As a result, HMD-based AR has primarily found applications in industrial settings, such as order picking and part assembly.
% However, HMD-based AR also introduces many technical complexities, such as making the hardware compact enough to be worn on the head, and designing and calibrating a display that effectively renders the composite image.

% For applications that do not require mobility, AR can alternatively be achieved using a static projector.
% Such a setup is not as constrained in terms of physical space, and simplifies the requirements of the display, since the superimposition of the image occurs physically.
% A common use-case for projected-based AR is the desk or workbench.
% DigitalDesk ..., perceptive workbench...

% Our work describes a hypothetical projected-based AR system for augmenting PCBs... explore the design space for faster interactions and information retrieval.

% \subsection{Visualization Tools for Breadboard Designs}
% \label{subsec: Visualization Tools for Breadboard Designs}



% \subsection{Augmenting Breadboards}
% \label{subsec: Augmenting Breadboards}
% \subsubsection{Visualization tools}
% Because of their easy solderless reconfigurability, breadboards are commonly used amongst hobbyists, students, designers, and engineers for prototyping simple, one-off circuits.
% Fritzing \cite{Knorig2009FritzingDesigners} was designed to support designers and artists in capturing breadboard designs virtually as well as translating breadboard designs into layouts.
% Important to the tool is the “breadboard view” which seeks to emulate the tangible experience of physically building circuits by having draggable wires and drop-in, semi-realistic components.
% Similar to how the layout file is a virtual spatial capture of a PCB, the Fritzing diagram serves as a “digital twin” to a real breadboard circuit, providing a virtual copy on which future tools add augmentation.

% % \subsection{Visualization Tools for Breadboard Designs Incorporating Interactivity and Measurement}
% % \label{subsec: Visualization Tools for Breadboard Designs Incorporating Interactivity and Measurement}
% \subsubsection{Adding interactivity and measurement}
% A number of systems have extended the virtual breadboard visualizations to allow for augmented interaction or measurement capability.
% Proxino \cite{Wu2019Proxino:Proxies} extends Fritzing by allowing for physically maniputable sensor proxies that can drive values in a virtual components within a  Fritzing circuit.
% Visible Breadboard \cite{Ochiai2014VisibleElectricity} allows for wiring to be performed with swipes directly on the board, and the connected nodes are highlighted in the same LED color.
% Toastboard \cite{Drew2016TheCircuits} and CurrentViz \cite{Wu2017CurrentViz:Circuits} collect real-time state information of prototyped breadboard circuits on custom instrumented breadboard hardware, specifically voltage and current information respectively.
% Toastboard pushes voltage information into a virtual diagram, and also provides row-associated status LEDs which were cited by all participants as being useful for debugging feedback. In the CurrentViz, current information from an instrumented breadboard is visualized atop a corresponding Fritzing diagram.

% While measurement points can be arbitrarily accessed in breadboarded setups, access points to pull current information must be designed into PCBs a priori, usually in the form of external sense resistors or internal sense resistors within PMICs (power management integrated circuits) or other ICs.
% RwmLAB \cite{Asumadu2003AInstrument}, Scanalog \cite{Strasnick2017Scanalog:Hardware}, VISIR \cite{Tawfik2013VirtualBreadboard} and VirtualComponent \cite{Kim2019VirtualComponent:Circuits} leverage this arbitrary circuit node access allowed by breadboarding to perform component insertion.
% Each tool has an associated breadboard GUI where users can design a virtual circuit and an instrumented breadboard is re-wired to match this digital twin. 
% Measurements can be taken on the physical circuits via the GUI.
% Scanalog and VirtualComponent both allow access to mix physical and virtual components, and VirtualComponent provides a mobile AR based overlay of the virtual components on a video feed of a physical breadboard.
% Each of these tools, while supporting the maker, student, or DIY hobbyist may not support the needs experience in the electronics industry.
% Breadboards are a quick and easy platform to iterate on the topology and values of simple analog and low-speed digital circuits.
% However in industry, circuits are rarely prototyped on breadboards as the circuits are too complex to hand-assemble into a breadboard format, many ICs are not available in DIP packages compatible with breadboarding, and parasitic capacitance in breadboard construction can affect sensitive or high speed signals. 

% \subsection{Visualization Tools for PCB Designs}
% \label{subsec: Visualization Tools for PCB Designs}
\subsection{Augmented Reality for PCBs}


While PCBs are often considered the staple of industry level electronics, breadboards are often used by students and hobbyists for their solderless reconfigurability that enables rapid iteration, and are rarely used as part of the hardware development process in industry.
While recent work in the HCI community aimed at the student population has demonstrated a number of breadboard augmentation techniques \cite{Ochiai2014VisibleElectricity, Drew2016TheToastboard, Wu2017CurrentViz, Kim2019VirtualComponent}, PCBs are substantially more intricate, requiring much more careful augmentation.
We motivate our work as a means toward this end.
In this section, we discuss more directly comparable related work in the realm of specifically augmenting PCBs.


\subsubsection{Visualization Tools}
A few tools support visualizing certain component metadata, such as location, directly on the PCB.
InspectAR \cite{InspectARTools} is a recently released tool that uses mobile AR to overlay elements of the layout and associated metadata onto a camera view of the PCB displayed on a mobile tablet or PC.
It is targeted toward supporting industry professionals, with couplings to industry standard ECAD tools.
The tool does not seem to support direct interaction with the PCB itself, measurement interactions, or a topological schematic view.
The sales webpage offers strong testimonials speaking to the increased assembly and debugging efficiency from decreased context-switching, claiming “an average 30\% reduction in lab-time.”
While these indications speak strongly to the hypothesis that mixed reality visualization of layout metadata on PCB can increase efficiency, a systematic study is yet to be published.
The Mascot \cite{MascotRobotas}, a robotic workbench from Robotas, helps to support operators performing hand assembly of through hole components
by steering a projected laser spot to the installation location on an anchored PCB.
%The tool allows for preloading of assembly steps, which controls automated picking carousels and a projected laser spot showing assembly position on an anchored PCB.
Similarly, Hahn et al. \cite{Hahn2015AugmentedProcess} generated an AR tool with textual and graphical cues delivered through a smartglass for assisting workers performing PCB assembly, indicating that the tool allowed for errorless part picking and assembly.
Hahn et al.’s tool, InspectAR and Mascot all provide board-locked augmented instruction for PCB workflow, driving information from the virtual design files to the user’s view of the PCB. Our work broadens the design space seeking to also incorporate augmented interaction and measurement to pass data in the opposite direction, that is, interactive capture in the PCB view can be passed to the virtual design files.

% \subsection{Visualization Tools for PCB Designs Incorporating Interactivity and Measurement}
% \label{subsec: Visualization Tools for PCB Designs Incorporating Interactivity and Measurement}
\subsubsection{Adding Interactivity and Measurement}
Pinpoint \cite{Strasnick2019Pinpoint} is a tool designed to assist in PCB debugging by allowing users to modify and measure the circuit \textit{in situ} after the PCB is fabricated. The tool modifies the layout of a PCB by inserting breakable connections on some traces. While not using augmented reality per se, the tool connects the virtual and the physical by using GUI-controlled relays to make and break these connections. For form factor designs and mass-produced PCBs, modifying the layout for test is typically restrained to adding test points on critical nets for bed-of-nails, on-line testing or manual access for workbench debugging. Our work seeks to support existing debugging workflows that do not modify the PCB design, and instead ease access to measurement points by guiding users with augmentations.
%Pinpoint \cite{Strasnick2019Pinpoint:Instrumentation} extends the concept in Scanalog, VISIR, and Virtual Component to PCB designs. The tool adds up to 16 jumper pads into a PCB design before fabrication, generates a bed-of-nails jig to interface with the jumper pads, and modifies connections through a GUI-controlled relay board. While the tool adds a unique layer of debugging capability akin to breadboarding -- the ability to access and isolate parts of a circuit for easy measurement or modification -- it necessitates amending the original design routing. For form factor designs and mass-produced PCBs, a high level of control in layout is maintained by the designer with board size, manufacturing, cost, and signal integrity concerns often precluding the use of an auto routing mechanism. The modifications to layout in production designs in support of test is therefore typically restrained to adding test points on critical nets for bed-of-nails, on-line testing or manual access for workbench debugging. Our work seeks to support existing debugging workflows that do not modify the PCB design, and instead ease access to measurement points by guiding users with augmentations.
More relevant to our work, BoardLab presents a magnetically tracked stylus that enables interactions from board to schematic, such as selecting and identifying components on the schematic by touching the components on the board as well as taking voltage measurements and having the measurement annotated on the schematic \cite{Goyal2013BoardLab}.
Although the system looks promising, no formal evaluation was reported.
Our work studies whether the interactions afforded by such a stylus would be helpful to electrical engineers, as well as exploring interactions that are synergistically enabled as augmented interaction and measurement is paired with simultaneous augmented visualization.

\subsection{Current Tools}

Electrical engineers use a diverse set of inspection tools across various stages of the design process. In the early engineering validation test phase, manual tools such as oscilloscopes, logic analyzers, and multimeters are used to provide measurements of signal properties on the physical board. For high volume production, select functions maybe be tested via automated in-circuit testing (ICT) equipment, a test machine that accesses a set of PCB test points via a custom spring-loaded pogo pin fixture, making ICT expensive and time-consuming to set up. While allowing for high-throughput testing, the test suite  only has access to the limited set of test points for diagnostic checks and is therefore used as an end-of-line functional check, rather than allowing for root causing and free-form debugging. In this paper, we focus on tools to enable better debugging for the early stages of development and validation. In this section, we first cover related work on enabling better debugging workflow on breadboards and later cover work related to extending the capabilities of PCBs.   

%Augmented Reality (AR) have the potential to be highly adaptable and customizable to individual users and specific use cases, while minimizing physical barriers between virtual and physical information transfer. %Recent work have demonstrated the feasibility and effectiveness of using AR to display information on PCBs~\cite{Chatterjee2021AugmentedBoards}. Augmented Silkscreen presented a set of augmented reality interaction techniques to assist electrical engineers in PCB debugging. They found that combining augmented visualization and augmented interaction on printed circuit boards unlocks promising avenues to alleviate the frequent context switching~\cite{Chatterjee2021AugmentedBoards}. In the reset of this section, we will first focus on related work on enabling better debugging workflow on breadboards and later we provide literature related to extending the capabilities of PCBs.

\subsection{Extending Breadboards}

Breadboards offer a gridded construction base in which electronic components can be inserted or removed.
Because of their easy solderless reconfigurability, breadboards are commonly used amongst hobbyists, students, designers, and engineers for prototyping simple, one-off circuits.
% They are primarily used for educational purposes.
Prior work has explored augmenting breadboards to extend their capabilities, primarily for enabling measurement and testing features.
Toastboard \cite{Drew2016TheToastboard},  CurrentViz \cite{Wu2017CurrentViz:Circuits}, and Heimdall~\cite{Karchemsky2019Heimdall:Projects} make real-time measurements on a circuit prototyped on an instrumented breadboard, specifically voltage and current information respectively, and show this information on a computer screen.
Toastboard additionally uses status LEDs on each row of the breadboard to provide localized feedback, a feature cited by all of their study participants as being useful during debugging.
In addition to making measurements, previous work has also explored adding \textit{in situ} reconfiguration and test capabilities to breadboards.
Visible Breadboard allows a user to create virtual wires by dragging a finger along the desired path on a breadboard \cite{Ochiai2014VisibleElectricity}.
A layer of relays under the breadboard reconfigures itself to form the desired electronic connection.
Proxino similarly enables reconfiguration of an underlying layer such that users can combine virtual and physical components \cite{Wu2019Proxino}.

Due to their modular nature, breadboards are helpful for rapid prototyping and primarily used for educational purposes.
However, they are often not practical in supporting the prototyping of circuits for commercial products, as the physical design of breadboards restricts the possible package choices and layouts of the electronic components and introduces contact resistance and parasitic capacitance that can affect sensitive or high speed analog signals.
Furthermore, replicating breadboards for more than a handful of prototypes can be tedious.
In our work, we seek to support engineers in academia and industry who generally develop PCBs for medium-volume development and mass production.

\subsection{Extending Printed Circuit Boards}
\subsubsection{Augmented Visualization on PCBs}
%A number of systems have extended the virtual PCB visualizations to allow for augmented interaction without focusing on measurement capabilities.
A number of systems incorporated virtual augmentations on PCBs in varying capacities.
InspectAR is a startup that has developed a tool using mobile AR to overlay elements of a board layout and associated metadata onto the camera's view of a PCB displayed on a mobile tablet or PC \cite{InspectARTools}. While serving detailed visualizations and component datasheets, the tool does not support
direct interaction with the PCB itself, measurement interactions,
or a topological schematic view for debugging. 
The Mascot, a robotic workbench from Robotas, helps to support operators performing hand assembly of through hole components \cite{MascotRobotas}.
The tool allows for preloading of assembly steps, which controls automated picking carousels and a projected laser spot showing assembly position on a clamped PCB.
Similarly, Hahn et al. \cite{Hahn2015AugmentedProcess} built an AR system that delivers textual and graphical cues through a head-mounted display for assisting workers performing PCB assembly, and indicates that the tool helped minimize error in part picking and assembly.
HolOsci \cite{Javaheri2018HolOsci} is a tool that incorporates voice commands and grabs an oscilloscope screen and overlays it onto the handheld probe to prevent users from slipping off their probe point when interacting directly with the instrument panel.

Hahn et al.’s tool, InspectAR, Mascot, and HolOsci all provide board-locked augmented instruction for PCB workflow, driving information from the virtual design files or instrument panel to the user’s view of the PCB.
Our work seeks to also incorporate augmented interaction and measurement to pass data in the opposite direction, that is, interactive capture directly on the physical PCB can be passed to the virtual design files to assist engineers in debugging.

\subsubsection{Augmented Interaction and Measurement on PCBs}
Pinpoint \cite{Strasnick2019Pinpoint} is a tool designed to assist in PCB debugging by adding up to 16 jumper pads into a PCB design before fabrication, generates a bed-of-nails jig to interface with the jumper pads, and modifies connections through a GUI-controlled relay board. While not using augmented reality per se, the tool connects the virtual and the physical by using GUI-controlled relays to make and break these connections. However, modifications to layout in acceptable in production designs in support of test is typically restricted to adding test points only on critical, low-speed nets for bed-of-nails ICT or manual access for workbench debugging. Further work by Stransnick et al. has  explored the utility of tightly coupling simulated and physical versions of the same circuit to support interactive debugging by leveraging instrumentation and programmable test hardware~\cite{Strasnick2021CouplingDebugging}. Our work seeks to support existing debugging workflows that do not modify the PCB design or add new test hardware, and instead ease access to measurement points by guiding users with augmentations.

Most relevant to our work is BoardLab which presents a magnetically tracked stylus that enables interactions from board to schematic, such as selecting and identifying components on the schematic by touching the components on the board as well as taking voltage measurements and having the measurement annotated on the schematic \cite{Goyal2013BoardLab}.
{Although the system looks promising, BoardLab does not comment on system accuracy, implement board tracking, or conduct a usability study of their device.}
{Our work fuses the augmented interaction demonstrated in BoardLab with augmented visualization on the PCB, and discusses the practical usability considerations of the system.} 

\subsubsection{{Combining Augmented Visualization and Interaction on PCBs}}

{In previous design work{\cite{Chatterjee2021AugmentedBoards}}, we presented a set of augmented reality \textit{interaction techniques}, called Augmented Silkscreen, to assist electrical engineers in PCB debugging. Through a set of remote design interviews, we found that \textit{combining} augmented visualization (as seen in works like InspectAR) and augmented interaction (as seen in works like BoardLab) on PCBs unlocks promising avenues to alleviate frequent context switching between schematic, layout, and physical PCB.
By having information flow bi-directionally between virtual design files and physical PCB, compelling interactions can be realized; for example, providing on-board augmentations to guide a user toward a measurement and then logging that measurement and its location. In this work, we build an end-to-end system that implements the interaction techniques proposed in Augmented Silkscreen. We run a user study to explore the efficacy of the system in practical scenarios.}

\section{Spatial Computing Sensing: Spatial Audio}

Endfire beamforming configurations remain popular on  consumer mobile phones and earbuds \cite{samsungglobalnewsroom_2014, airpods, sennheiser_2020, beamforming-app-note}. While recent advances in neural networks have shown promising results, none of them are demonstrated with  wireless earbuds.  By creating a wireless   network between two earbuds, we demonstrate that our real-time, two-channel neural network can outperform  current real-time speech enhancement approaches for wireless earbuds. %Below, we briefly discuss beamforming, single channel speech enhancement, and binaural networks.

%A common approach to enhancing speech is to design a beamforming microphone array to be more sensitive to sounds coming from the direction of the user's mouth \cite{van1988beamforming} or voice~\cite{dov-uist21}.

\subsubsection{Beamforming techniques.}  Since signal-processing based beamforming is computationally lightweight, these techniques are deployed on  commercial devices such as smart speakers \cite{amazon}, mobile phones \cite{samsungglobalnewsroom_2014}, and earbud devices like Apple AirPods \cite{airpods}. However, the performance of beamforming is limited by the geometry of the microphones and the distance between them \cite{van1988beamforming, InvenSense}. The form factor of devices like AirPods restricts both the number of microphones on a single earbud and the available distance between them, limiting the gain of the beamformer. While beamforming  across two earbuds could provide better performance in principle, current wireless architectures are limited to streaming from a single earbud at a time \cite{bluetooth}. Furthermore, adaptive beamformers such as MVDR \cite{frost1972MVDR}, while showing promise with relatively few interfering sources,  are sensitive to sensor placement tolerance and steering \cite{zhang2017deep, brandstein2001microphone}. Finally, beamforming leverages spatial or spectral cues only and does not use acoustic cues (e.g., structure in human speech) and perceptual differences to discriminate sources, information that machine learning methods leverage successfully.

% Don H. Johnson and Dan E. Dudgeon. Array Signal Processing: Concepts and Techniques. Simon & Schuster, Inc., USA, 1992

% Beamforming microphone arrays for speech enhancement, https://ieeexplore.ieee.org/abstract/document/225915

% Rate-Constrained Beamforming in Binaural Hearing Aids
% https://link.springer.com/content/pdf/10.1155/2009/257197.pdf

% Dual-Channel Speech Enhancement by
% Superdirective Beamforming
% https://link.springer.com/content/pdf/10.1155/ASP/2006/63297.pdf


%\vskip 0.05in\noindent{\bf Single-channel speech enhancement.} 
\subsubsection{Single-channel deep speech enhancement.}
Many deep learning techniques operate on spectrograms to separate the human voice from background noise \cite{realtimenoise, Mohammadiha_2013, online_nonnegative, nikzad2020deep, choi2019phaseaware, lstm_speechenhancement, fu2019metricgan, TFMasking}. However, recent works  instead operate directly on the time domain signals \cite{luo2019conv, germain2018speech, pascual2017segan, demucsreal, macartney2018improved}, yielding performance improvements over spectrogram approaches. Commercial noise suppression software like Krisp \cite{krisp} and Google Meet \cite{googlemeet} have successfully deployed single-channel models in real-time and are available for use on mobile phones and desktop computers, but {processing is performed on the cloud.}~\cite{tinylstm}  achieves low-power speech enhancement using
long short-term memory (LSTM), but it is for a  single-channel network but not for multichannel source separation. Further, single-channel models cannot effectively capture  spatial information and fail to isolate the intended speaker when there are multiple speakers (see Fig.~\ref{fig:fig3}).

% --> some recent work on increasing performance to real-time applications
% Real Time Speech Enhancement in the Waveform Domain
% https://arxiv.org/abs/2006.12847



\subsubsection{Multi-channel source separation and speech enhancement.}
%\subsection{Multi-channel source separation and speech enhancement}
Multi-channel methods have been shown to perform better than their single-channel source separation counterparts \cite{yoshioka2018multi, chen2018multi, zhang2017deep, gu2020enhancing, tzirakis2021multichannel, jenrungrot2020cone}. Binaural methods have also been used for source separation \cite{binaural1, han2020realtime, li2011two, reindl2010speech} and localization \cite{van2008binaural, lyon1983computational, kock1950binaural}; \cite{han2020realtime} reduces the look-ahead time in the network to make it causal in behavior but has not been demonstrated to run on a mobile device.  Our method improves on existing binaural methods by combining time-domain neural network with spectrogram-based frequency masking networks as well as optimizing them to enable   real-time processing on a phone. Recent works such as ~\cite{binaural_osu, dual_phone,binauralphone} use multiple microphones on a smartphone for  speech-enhancement. However, neither of them demonstrates evaluation with real data, where artifacts because of network optimizations can affect user performance.
In contrast, we demonstrate the first system that achieves real-time speech enhancement using microphones on the two wireless earbuds. Further, as the distance between the earbuds is larger than the distance between microphones on a typical mobile phone, we can  attain a better baseline than a mobile phone implementation, while also retaining the ability to speak hands-free. More recent  works tackle the problem of real-time directional hearing using eye trackers and wearable headsets. For example,  \cite{hybridbeam} uses a hybrid network that combines signal processing with neural networks, but shows that their technique performs poorly in binaural scenarios (i.e., two microphones) and requires four or more microphones. In contrast, we focus on the problem of speech enhancement and create the first real-time end-to-end hardware-software neural-network based system using wireless synchronized earbuds. 

\subsubsection{Earbud  computing and platforms.}
There has been recent interest in earbud computing~\cite{oesense21,esense-1,esense-2,plat-1,romit-1} to address  applications in health monitoring~\cite{infection,tam1,tymp}, activity tracking~\cite{mobisys21} and sensor fusion with EEG signals~\cite{eeg2}. The  eSense platform~\cite{esense-1,esense-2} has enabled research in   sensing applications with earables. OpenMHA~\cite{open-1,open-2} is an open signal processing {\it software} platform for hearing aid research. Neither of these  platforms   support time-synchronized audio transmission from two earbuds, which is a critical requirement for achieving  speech enhancement in binaural settings. In contrast, we created open-source wireless earbud hardware that can support synchronize wireless transmission from the two earbuds. 

\section{Spatial Computing Control: Microgesture Interaction}


Commercial hand gesture detection systems \cite{microsoft, MetaStore, TrackingUltraleap} often rely on optical methods, using cameras mounted on external devices such as AR/VR headsets or necklaces. However, these systems require a clear line of sight to the hand, complicating efforts to  detect gestures made outside the camera's field of view.
% Therefore, a number of systems have explored sensors mounted on the wrist, hand, and/or fingers, spanning a wide range of modalities: \red{optical (Digits \cite{Kim2012Digits},
% TouchPoint \cite{Chatterjee2016TouchPoint:Device},
% SensIR \cite{McIntosh2017SensIR:Reflection}, CyclopsRing \cite{Chan2015Cyclopsring:Ring}, PinchWatch \cite{loclair2010pinchwatch}), thermal (Pyro \cite{Gong2017Pyro:Sensing}, ThermalRing \cite{thermalring}), bio-acoustic (Amento et al. \cite{10.1145/506443.506566}, 
% Mujibiya et al. \cite{sot}, 
% FingerPing \cite{fingerping}), 
% ultrasonic (Beamband \cite{Iravantchi2019BeamBand:Beamforming}, SoundTrak \cite{Zhang2017SoundTrak}), mechanical (DataGlove \cite{Takada2019AFiber}, BackHand \cite{Lin2015BackHand:Hand}, 
% Kuno et al. \cite{10.5555/3298830.3298870}), 
% and inertial (Gu et al. \cite{Guy}, TapID \cite{Meier2021TaplD:Sensing}, FingeRing \cite{Fukumoto1994FingeRing:Interface}, DualRing \cite{Liang2021DualRing}).}
Therefore, many systems have explored sensors mounted on the wrist, hand, and/or fingers, spanning a wide range of modalities: \red{optical (\cite{Chatterjee2016TouchPoint:Device}, \cite{Kim2012Digits},
\cite{McIntosh2017SensIR:Reflection}, \cite{Chan2015Cyclopsring:Ring},  \cite{loclair2010pinchwatch}), (\cite{Gong2017Pyro:Sensing}, \cite{thermalring}), bio-acoustic (\cite{10.1145/506443.506566}, \cite{sot}, \cite{fingerping}), 
ultrasonic (\cite{Iravantchi2019BeamBand:Beamforming}, \cite{Zhang2017SoundTrak}), mechanical (\cite{Takada2019AFiber}, \cite{Lin2015BackHand:Hand}, \cite{10.5555/3298830.3298870}), 
and inertial (\cite{Guy}, \cite{Meier2021TaplD:Sensing}, \cite{Fukumoto1994FingeRing:Interface}, \cite{Liang2021DualRing}).}
The capability of these devices ranges from detecting pinches to recognizing discrete gesture sets to driving full kinematic models.

However, these approaches have several limitations: (1) passive IMUs and bio-acoustic techniques can detect when the fingers make contact  but not when they release, which is important for actions like dragging and dropping; (2) IMUs used for gesture sensing require multiple points of instrumentation, making the system awkward to use; (3) optical and ultrasonic techniques require a clear line of sight to each gesturing appendage, limiting possible mounting positions and making it difficult to detect pinches; and (4) mechanical and magnetic systems require instrumentation of the whole hand, back of the hand, or fingertips, making the system uncomfortable to use.
%The limitations of these objects can be roughly categorized into a few areas: (1) passive IMU and bio-acoustic techniques can detect pinch touchdowns, however cannot detect when the fingers release, an important factor as a clutching mechanism, for example, drag-and-drop, (2) when used for gesture sensing, IMUs require multiple instrumentation points yielding an awkward system, (3) optical and ultrasonic techniques require line-of-sight to each of the gesturing appendages resulting non-ideal mounting positions to capture the full gesture set and are challenged to reliably detect pinches, (4) mechanical and magnetic systems require instrumenting the whole hand, back-of-the-hand, or fingertips, yielding an uncomfortable system.
In contrast, Z-Ring instruments the body at only a single location, i.e., the base of the index finger, an ergonomic and socially acceptable area for worn systems. \red{By using the body as a transmission medium, Z-Ring does not require line-of-sight for microgesture sensing. In addition, it can robustly sense both touchdown and touch up events for one- and two-handed gestures even if touch velocity is low.}
%This motivated other optical methods to  explore mounting on the arm or finger to detect gestures. Digits derives an articulated hand model leveraging a wirst-mounted IR camera and projector \cite{Kim2012Digits}. TouchPoint uses a pair of line sensors for continuous finger tracking on the back-of-the-hand \cite{Chatterjee2016TouchPoint:Device}. Back-Hand-See . CyclopsRing enables gesture sensing an \cite{}.
 
% ========== Chapter 3
 
\chapter{Augmented Task Guidance: AR Debugging Workbench}

% ========== Chapter 4

\chapter{Spatial Audio: ClearBuds}

% ========== Chapter 5

\chapter{Microgesture Interaction: ToFRing and Placeholder}





%\printendnotes
%
% ==========   Bibliography
%
\nocite{*}   % include everything in the uwthesis.bib file
\bibliographystyle{plain}
\bibliography{uwthesis}
%
% ==========   Appendices
%
\appendix
\raggedbottom\sloppy
 
% ========== Appendix A
 
\chapter{Where to find the files}
 


\end{document}
